{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","mount_file_id":"1HzM8E7yV6kUMK2HJP9K16Tqys7anJHSQ","authorship_tag":"ABX9TyMNYctR/U9GyRjzHwYNAlf2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQruOJkW5-dK","executionInfo":{"status":"ok","timestamp":1759258871627,"user_tz":300,"elapsed":6765,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}},"outputId":"18349d89-01de-4b0a-8615-8d91b2d60bd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q segmentation-models-pytorch"]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import segmentation_models_pytorch as smp\n","from PIL import Image\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import random\n"],"metadata":{"id":"aH65wpG96MRx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Config:\n","    # -- Data Paths --\n","    DRIVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/\"\n","    IMAGE_DIR = os.path.join(DRIVE_PATH, \"images\")\n","    MASK_DIR = os.path.join(DRIVE_PATH, \"masks\")\n","\n","    # -- Output Paths --\n","    # Folders to save the predicted masks\n","    OUTPUT_DIR = os.path.join(DRIVE_PATH, \"outputs\")\n","    # Grayscale masks (for loss calculation & metrics)\n","    OUTPUT_MASK_DIR = os.path.join(OUTPUT_DIR, \"pred_masks\")\n","    # Colorized masks (for easy visualization)\n","    COLOR_MASK_DIR = os.path.join(OUTPUT_DIR, \"color_masks\")\n","\n","    # -- Model Hyperparameters --\n","    # 'unet' architecture\n","    ARCHITECTURE = 'unet'\n","    # 'resnet34' as encoder\n","    ENCODER = 'resnet34'\n","    ENCODER_WEIGHTS = 'imagenet'\n","    LEARNING_RATE = 1e-4\n","    # 'DiceLoss'\n","    LOSS_FUNCTION = 'DiceLoss'\n","    OPTIMIZER = 'AdamW'\n","    #TRAIN_SIZE + VAL_SIZE + TEST_SIZE <= total images.\n","    TRAIN_SIZE = 412  # Number of images for training\n","    VAL_SIZE = 51    # Number of images for validation\n","    TEST_SIZE = 52   # Number of images for final testing\n","    # -- Training Settings --\n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    BATCH_SIZE = 4\n","    NUM_EPOCHS = 25\n","    IMAGE_HEIGHT = 256\n","    IMAGE_WIDTH = 256\n","    NUM_CLASSES = 5\n","\n","    # -- Visualization --\n","    # This map is used ONLY for creating the colorized masks for visualization\n","    COLOR_MAP = {\n","        0: (0, 0, 0),        # background (black)\n","        1: (0, 255, 0),      # stem (green)\n","        2: (255, 255, 0),    # leaf (yellow)\n","        3: (139, 69, 19),    # root (brown)\n","        4: (255, 255, 255),  # seed (white)\n","    }\n","\n","# Create output directories if they don't exist\n","os.makedirs(Config.OUTPUT_MASK_DIR, exist_ok=True)\n","os.makedirs(Config.COLOR_MASK_DIR, exist_ok=True)\n"],"metadata":{"id":"Xv21g0IJ64B6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PlantDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, image_filenames, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.images = image_filenames\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        img_name = self.images[index]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        mask_name = os.path.splitext(img_name)[0] + \"_mask.png\"\n","        mask_path = os.path.join(self.mask_dir, mask_name)\n","\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n","\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        return image, mask.long()\n","\n","# Define augmentations.\n","!pip install -q albumentations\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# Augmentations for the training set\n","train_transform = A.Compose([\n","    A.Resize(height=Config.IMAGE_HEIGHT, width=Config.IMAGE_WIDTH),\n","    A.Rotate(limit=35, p=0.5),\n","    A.HorizontalFlip(p=0.5),\n","    A.VerticalFlip(p=0.5),\n","    A.Normalize(\n","        mean=[0.0, 0.0, 0.0],\n","        std=[1.0, 1.0, 1.0],\n","        max_pixel_value=255.0,\n","    ),\n","    ToTensorV2(),\n","])\n","\n","# For validation, we only resize and normalize\n","val_transform = A.Compose([\n","    A.Resize(height=Config.IMAGE_HEIGHT, width=Config.IMAGE_WIDTH),\n","    A.Normalize(\n","        mean=[0.0, 0.0, 0.0],\n","        std=[1.0, 1.0, 1.0],\n","        max_pixel_value=255.0,\n","    ),\n","    ToTensorV2(),\n","])"],"metadata":{"id":"LSM41o9fAYBM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mask_to_rgb(mask_tensor, color_map):\n","    \"\"\"Converts a segmentation mask (tensor) to a colorized RGB image.\"\"\"\n","    mask = mask_tensor.cpu().numpy().squeeze()\n","    rgb_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n","    for class_idx, color in color_map.items():\n","        rgb_mask[mask == class_idx] = color\n","    return Image.fromarray(rgb_mask)"],"metadata":{"id":"A1-yWQDwAca9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_predictions_fn(loader, model, folder_basename=\"\"):\n","    \"\"\"Saves model predictions on a given dataset to the disk.\"\"\"\n","    print(f\"\\n--- Saving predictions for {folder_basename} set ---\")\n","    model.eval()\n","\n","    # Create specific subdirectories for train/val/test predictions\n","    output_mask_dir = os.path.join(Config.OUTPUT_MASK_DIR, folder_basename)\n","    color_mask_dir = os.path.join(Config.COLOR_MASK_DIR, folder_basename)\n","    os.makedirs(output_mask_dir, exist_ok=True)\n","    os.makedirs(color_mask_dir, exist_ok=True)\n","\n","    for idx, (img_tensor, _) in enumerate(tqdm(loader.dataset, desc=f\"Saving {folder_basename} Predictions\")):\n","        with torch.no_grad():\n","            # The dataset returns single images, so we add a batch dimension\n","            img_tensor = img_tensor.to(Config.DEVICE).unsqueeze(0)\n","            preds = model(img_tensor)\n","            final_mask = torch.argmax(preds, dim=1).squeeze(0)\n","\n","        # Save the raw integer mask\n","        pred_mask_img = Image.fromarray(final_mask.cpu().numpy().astype(np.uint8))\n","        original_filename = loader.dataset.images[idx]\n","        pred_mask_img.save(os.path.join(output_mask_dir, original_filename))\n","\n","        # Save the color mask\n","        color_mask_img = mask_to_rgb(final_mask, Config.COLOR_MAP)\n","        color_mask_img.save(os.path.join(color_mask_dir, original_filename))\n","    model.train()"],"metadata":{"id":"DuXSNCm4D68e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_fn(loader, model, optimizer, loss_fn, scaler):\n","    \"\"\"The training loop for one epoch.\"\"\"\n","    loop = tqdm(loader, desc=\"Training\")\n","    total_loss = 0\n","\n","    for batch_idx, (data, targets) in enumerate(loop):\n","        data = data.to(device=Config.DEVICE)\n","        targets = targets.to(device=Config.DEVICE).unsqueeze(1)\n","\n","        # Forward pass\n","        with torch.cuda.amp.autocast():\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","        loop.set_postfix(loss=loss.item())\n","\n","    return total_loss / len(loader)\n","\n","def eval_fn(loader, model, loss_fn):\n","    \"\"\"The evaluation loop.\"\"\"\n","    model.eval()\n","    total_loss = 0\n","    loop = tqdm(loader, desc=\"Validation\")\n","\n","    with torch.no_grad():\n","        for data, targets in loop:\n","            data = data.to(device=Config.DEVICE)\n","            targets = targets.to(device=Config.DEVICE).unsqueeze(1)\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","            total_loss += loss.item()\n","            loop.set_postfix(val_loss=loss.item())\n","\n","    model.train()\n","    return total_loss / len(loader)"],"metadata":{"id":"gvga2mj2AiQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    all_entries = sorted(os.listdir(Config.IMAGE_DIR))\n","    all_images = [entry for entry in all_entries if os.path.isfile(os.path.join(Config.IMAGE_DIR, entry))]\n","\n","    random.seed(42) # for reproducibility\n","    random.shuffle(all_images)\n","\n","    total_size = Config.TRAIN_SIZE + Config.VAL_SIZE + Config.TEST_SIZE\n","    if total_size > len(all_images):\n","        raise ValueError(\"Sum of split sizes is larger than the total number of images!\")\n","\n","    train_files = all_images[:Config.TRAIN_SIZE]\n","    val_files = all_images[Config.TRAIN_SIZE : Config.TRAIN_SIZE + Config.VAL_SIZE]\n","    test_files = all_images[Config.TRAIN_SIZE + Config.VAL_SIZE : total_size]\n","\n","    print(f\"Total images (after filtering): {len(all_images)}\")\n","    print(f\"Training set size: {len(train_files)}\")\n","    print(f\"Validation set size: {len(val_files)}\")\n","    print(f\"Test set size: {len(test_files)}\")\n","    # --- Create Model ---\n","    model = smp.create_model(\n","        arch=Config.ARCHITECTURE,\n","        encoder_name=Config.ENCODER,\n","        encoder_weights=Config.ENCODER_WEIGHTS,\n","        in_channels=3,\n","        classes=Config.NUM_CLASSES,\n","    ).to(Config.DEVICE)\n","\n","    # --- Select Loss Function ---\n","    if Config.LOSS_FUNCTION == 'DiceLoss':\n","        loss_fn = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n","    elif Config.LOSS_FUNCTION == 'FocalLoss':\n","        loss_fn = smp.losses.FocalLoss(mode='multiclass')\n","    else: # Default to CrossEntropy\n","        loss_fn = nn.CrossEntropyLoss()\n","\n","    # --- 3. Select Optimizer ---\n","    if Config.OPTIMIZER == 'AdamW':\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n","    else: # Default to Adam\n","        optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n","\n","    # --- Create Datasets and Dataloaders ---\n","    train_dataset = PlantDataset(\n","        image_dir=Config.IMAGE_DIR, mask_dir=Config.MASK_DIR,\n","        image_filenames=train_files, transform=train_transform\n","    )\n","    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n","\n","    val_dataset = PlantDataset(\n","        image_dir=Config.IMAGE_DIR, mask_dir=Config.MASK_DIR,\n","        image_filenames=val_files, transform=val_transform\n","    )\n","    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n","\n","    test_dataset = PlantDataset(\n","        image_dir=Config.IMAGE_DIR, mask_dir=Config.MASK_DIR,\n","        image_filenames=test_files, transform=val_transform\n","    )\n","    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False) # Batch size 1 for saving\n","\n","    # --- Start Training ---\n","    scaler = torch.cuda.amp.GradScaler()\n","    best_val_loss = float('inf')\n","\n","    for epoch in range(Config.NUM_EPOCHS):\n","        print(f\"\\n--- Epoch {epoch+1}/{Config.NUM_EPOCHS} ---\")\n","        train_loss = train_fn(train_loader, model, optimizer, loss_fn, scaler)\n","        val_loss = eval_fn(val_loader, model, loss_fn)\n","\n","        print(f\"Average Train Loss: {train_loss:.4f}\")\n","        print(f\"Average Val Loss: {val_loss:.4f}\")\n","\n","        # Save model if validation loss improves\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), os.path.join(Config.DRIVE_PATH, \"best_model.pth\"))\n","            print(\"=> Saved new best model\")\n","\n","    # --- Save Predictions After Training ---\n","    print(\"\\n--- Saving predictions ---\")\n","    model.load_state_dict(torch.load(os.path.join(Config.DRIVE_PATH, \"best_model.pth\")))\n","    model.eval()\n","\n","    # Save predictions for the validation set\n","    save_predictions_fn(val_loader, model, folder_basename=\"validation\")\n","    # Save predictions for the test set\n","    save_predictions_fn(test_loader, model, folder_basename=\"test\")"],"metadata":{"id":"PuB5vWphAswa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0v3AtFVA0IA","executionInfo":{"status":"ok","timestamp":1759261797549,"user_tz":300,"elapsed":236143,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}},"outputId":"0367d645-49be-431b-8c63-a5696914998c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total images (after filtering): 515\n","Training set size: 412\n","Validation set size: 51\n","Test set size: 52\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2988588827.py:66: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Epoch 1/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training:   0%|          | 0/103 [00:00<?, ?it/s]/tmp/ipython-input-1664722249.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Training: 100%|██████████| 103/103 [00:43<00:00,  2.35it/s, loss=0.775]\n","Validation: 100%|██████████| 13/13 [00:05<00:00,  2.53it/s, val_loss=0.766]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.8293\n","Average Val Loss: 0.7719\n","=> Saved new best model\n","\n","--- Epoch 2/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.66it/s, loss=0.624]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.06it/s, val_loss=0.605]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.6892\n","Average Val Loss: 0.6201\n","=> Saved new best model\n","\n","--- Epoch 3/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 15.71it/s, loss=0.457]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.87it/s, val_loss=0.495]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.5581\n","Average Val Loss: 0.4926\n","=> Saved new best model\n","\n","--- Epoch 4/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 15.68it/s, loss=0.426]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.36it/s, val_loss=0.455]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.4611\n","Average Val Loss: 0.4328\n","=> Saved new best model\n","\n","--- Epoch 5/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 15.96it/s, loss=0.479]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.83it/s, val_loss=0.384]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.3980\n","Average Val Loss: 0.3768\n","=> Saved new best model\n","\n","--- Epoch 6/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 15.68it/s, loss=0.371]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 28.18it/s, val_loss=0.263]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.3385\n","Average Val Loss: 0.2831\n","=> Saved new best model\n","\n","--- Epoch 7/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:08<00:00, 12.36it/s, loss=0.283]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 24.80it/s, val_loss=0.264]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.2797\n","Average Val Loss: 0.2596\n","=> Saved new best model\n","\n","--- Epoch 8/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.64it/s, loss=0.222]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.34it/s, val_loss=0.214]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.2490\n","Average Val Loss: 0.2376\n","=> Saved new best model\n","\n","--- Epoch 9/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 15.87it/s, loss=0.188]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.80it/s, val_loss=0.198]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.2370\n","Average Val Loss: 0.2249\n","=> Saved new best model\n","\n","--- Epoch 10/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:08<00:00, 12.43it/s, loss=0.239]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.26it/s, val_loss=0.187]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.2214\n","Average Val Loss: 0.2040\n","=> Saved new best model\n","\n","--- Epoch 11/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.40it/s, loss=0.238]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.86it/s, val_loss=0.216]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.2191\n","Average Val Loss: 0.2230\n","\n","--- Epoch 12/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:08<00:00, 12.62it/s, loss=0.254]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 28.04it/s, val_loss=0.22]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.2111\n","Average Val Loss: 0.2177\n","\n","--- Epoch 13/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 14.92it/s, loss=0.197]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 28.11it/s, val_loss=0.186]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.2082\n","Average Val Loss: 0.2079\n","\n","--- Epoch 14/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.27it/s, loss=0.172]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.86it/s, val_loss=0.218]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.2021\n","Average Val Loss: 0.2011\n","=> Saved new best model\n","\n","--- Epoch 15/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.57it/s, loss=0.203]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.27it/s, val_loss=0.194]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1976\n","Average Val Loss: 0.1981\n","=> Saved new best model\n","\n","--- Epoch 16/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.08it/s, loss=0.276]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.86it/s, val_loss=0.209]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1990\n","Average Val Loss: 0.2023\n","\n","--- Epoch 17/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:08<00:00, 12.70it/s, loss=0.267]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 28.87it/s, val_loss=0.191]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1945\n","Average Val Loss: 0.1914\n","=> Saved new best model\n","\n","--- Epoch 18/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.12it/s, loss=0.219]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.55it/s, val_loss=0.186]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1908\n","Average Val Loss: 0.1998\n","\n","--- Epoch 19/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:08<00:00, 12.54it/s, loss=0.195]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.55it/s, val_loss=0.185]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1898\n","Average Val Loss: 0.1949\n","\n","--- Epoch 20/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.59it/s, loss=0.18]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.79it/s, val_loss=0.19]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1876\n","Average Val Loss: 0.1952\n","\n","--- Epoch 21/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.53it/s, loss=0.171]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 28.19it/s, val_loss=0.188]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1858\n","Average Val Loss: 0.1971\n","\n","--- Epoch 22/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.54it/s, loss=0.163]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.80it/s, val_loss=0.171]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1813\n","Average Val Loss: 0.1827\n","=> Saved new best model\n","\n","--- Epoch 23/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.51it/s, loss=0.189]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.55it/s, val_loss=0.193]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1836\n","Average Val Loss: 0.2199\n","\n","--- Epoch 24/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:08<00:00, 12.72it/s, loss=0.157]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 28.06it/s, val_loss=0.164]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1801\n","Average Val Loss: 0.1854\n","\n","--- Epoch 25/25 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 103/103 [00:06<00:00, 16.81it/s, loss=0.13]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.68it/s, val_loss=0.163]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.1794\n","Average Val Loss: 0.1911\n","\n","--- Saving predictions ---\n","\n","--- Saving predictions for validation set ---\n"]},{"output_type":"stream","name":"stderr","text":["Saving validation Predictions: 100%|██████████| 51/51 [00:01<00:00, 35.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Saving predictions for test set ---\n"]},{"output_type":"stream","name":"stderr","text":["Saving test Predictions: 100%|██████████| 52/52 [00:06<00:00,  8.23it/s]\n"]}]},{"cell_type":"markdown","source":["UNSQUEEZING"],"metadata":{"id":"Epr5tn1YeBmt"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","\n","def resize_saved_masks(\n","    original_image_dir,\n","    predicted_mask_dir,\n","    predicted_color_mask_dir,\n","    output_dir,\n","    output_color_dir\n","):\n","    \"\"\"\n","    Resizes saved data masks and color masks back to their original dimensions.\n","\n","    Args:\n","    - original_image_dir (str): Directory containing original images.\n","    - predicted_mask_dir (str): Directory containing predicted masks.\n","    - predicted_color_mask_dir (str): Directory containing predicted color masks.\n","    \"\"\"\n","    print(f\"Resizing masks from: {predicted_mask_dir}\")\n","    print(f\"Saving unsqueezed masks to: {output_dir}\")\n","    print(f\"Resizing color masks from: {predicted_color_mask_dir}\")\n","    print(f\"Saving unsqueezed color masks to: {output_color_dir}\")\n","\n","    # Create the output directories if they don't exist\n","    os.makedirs(output_dir, exist_ok=True)\n","    os.makedirs(output_color_dir, exist_ok=True)\n","\n","    pred_masks = os.listdir(predicted_mask_dir)\n","\n","    for mask_filename in tqdm(pred_masks, desc=\"Resizing all masks\"):\n","        mask_basename = os.path.splitext(mask_filename)[0]\n","\n","        # Find the corresponding original image to get its dimensions\n","        original_img_filename = \"\"\n","        for fname in os.listdir(original_image_dir):\n","            if fname.startswith(mask_basename):\n","                original_img_filename = fname\n","                break\n","\n","        if not original_img_filename:\n","            print(f\"  - Warning: Could not find original image for mask: {mask_filename}\")\n","            continue\n","\n","        try:\n","            # --- Get original dimensions ---\n","            original_img_path = os.path.join(original_image_dir, original_img_filename)\n","            original_img = Image.open(original_img_path)\n","            original_w, original_h = original_img.size\n","\n","            # --- 1. Process the DATA MASK ---\n","            pred_mask_path = os.path.join(predicted_mask_dir, mask_filename)\n","            predicted_mask = cv2.imread(pred_mask_path, cv2.IMREAD_UNCHANGED)\n","            resized_mask = cv2.resize(\n","                predicted_mask, (original_w, original_h), interpolation=cv2.INTER_NEAREST\n","            )\n","            output_mask_path = os.path.join(output_dir, mask_filename)\n","            cv2.imwrite(output_mask_path, resized_mask)\n","\n","            # --- 2. Process the COLOR MASK --- ## <<< ADDED\n","            pred_color_path = os.path.join(predicted_color_mask_dir, mask_filename)\n","            if os.path.exists(pred_color_path):\n","                predicted_color_mask = cv2.imread(pred_color_path)\n","                # Use a smoother interpolation for better visual quality\n","                resized_color_mask = cv2.resize(\n","                    predicted_color_mask, (original_w, original_h), interpolation=cv2.INTER_LINEAR\n","                )\n","                output_color_path = os.path.join(output_color_dir, mask_filename)\n","                cv2.imwrite(output_color_path, resized_color_mask)\n","\n","        except Exception as e:\n","            print(f\"  - Error processing {mask_filename}: {e}\")\n","\n","    print(\"Done resizing all masks!\")\n","\n","\n","ORIGINAL_IMAGES_DIR = Config.IMAGE_DIR\n","SAVED_OUTPUTS_DIR = Config.OUTPUT_DIR\n","UNSQUEEZED_OUTPUTS_DIR = os.path.join(Config.OUTPUT_DIR, \"Unsqueezed\")\n","\n","SAVED_PRED_MASKS_DIR = os.path.join(SAVED_OUTPUTS_DIR, \"pred_masks\")\n","\n","SAVED_COLOR_MASKS_DIR = os.path.join(SAVED_OUTPUTS_DIR, \"color_masks\")\n","\n","UNSQUEEZED_MASKS_DIR = os.path.join(UNSQUEEZED_OUTPUTS_DIR, \"pred_masks\")\n","\n","UNSQUEEZED_COLOR_MASKS_DIR = os.path.join(UNSQUEEZED_OUTPUTS_DIR, \"color_masks\")\n","\n","resize_saved_masks(\n","    original_image_dir=ORIGINAL_IMAGES_DIR,\n","    predicted_mask_dir=os.path.join(SAVED_PRED_MASKS_DIR, \"test\"),\n","    predicted_color_mask_dir=os.path.join(SAVED_COLOR_MASKS_DIR, \"test\"),\n","    output_dir=os.path.join(UNSQUEEZED_MASKS_DIR, \"test\"),\n","    output_color_dir=os.path.join(UNSQUEEZED_COLOR_MASKS_DIR, \"test\")\n",")\n","\n","resize_saved_masks(\n","     original_image_dir=ORIGINAL_IMAGES_DIR,\n","     predicted_mask_dir=os.path.join(SAVED_PRED_MASKS_DIR, \"validation\"),\n","     predicted_color_mask_dir=os.path.join(SAVED_COLOR_MASKS_DIR, \"validation\"),\n","     output_dir=os.path.join(UNSQUEEZED_MASKS_DIR, \"validation\"),\n","     output_color_dir=os.path.join(UNSQUEEZED_COLOR_MASKS_DIR, \"validation\")\n",")"],"metadata":{"id":"jKRz15UAGErE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Analysis of above model's performance:"],"metadata":{"id":"dkBTQf2moRsp"}},{"cell_type":"code","source":["# =================================================================================\n","# 0. SETUP AND IMPORTS\n","# =================================================================================\n","!pip install -q monai pandas\n","\n","import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","from tqdm import tqdm\n","from monai.metrics import (\n","    compute_dice,\n","    compute_iou,\n","    compute_hausdorff_distance\n",")\n","\n","# =================================================================================\n","# 1. CONFIGURATION\n","# =================================================================================\n","# Path to your ORIGINAL ground truth masks\n","GT_MASK_DIR = \"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/masks/\"\n","\n","# Path to the UNSQUEEZED predicted masks from your test set\n","PRED_MASK_DIR = \"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/outputs/Unsqueezed/pred_masks/test/\"\n","\n","# --- CLASS DEFINITIONS ---\n","NUM_CLASSES = 5\n","CLASS_MAP = {\n","    0: \"Background\",\n","    1: \"Stem\",\n","    2: \"Leaf\",\n","    3: \"Root\",\n","    4: \"Seed\",\n","}\n","\n","# =================================================================================\n","# 2. HELPER FUNCTION (Converts masks to one-hot format)\n","# =================================================================================\n","def to_one_hot(mask, num_classes):\n","    \"\"\"\n","    Converts a (H, W) segmentation mask with class indices\n","    to a (C, H, W) one-hot encoded tensor.\n","    \"\"\"\n","    # (H, W, C)\n","    one_hot = np.eye(num_classes)[mask]\n","    # (C, H, W)\n","    one_hot = np.transpose(one_hot, (2, 0, 1))\n","    return torch.from_numpy(one_hot)\n","\n","# =================================================================================\n","# 3. MAIN ANALYSIS LOOP\n","# =================================================================================\n","\n","print(\"Starting analysis...\")\n","print(f\"GT Directory: {GT_MASK_DIR}\")\n","print(f\"Pred Directory: {PRED_MASK_DIR}\")\n","\n","# A list to store the metric results for each image\n","results_list = []\n","\n","# Get the list of predicted files\n","pred_files = os.listdir(PRED_MASK_DIR)\n","\n","for filename in tqdm(pred_files):\n","    # Construct full paths\n","    pred_path = os.path.join(PRED_MASK_DIR, filename)\n","\n","    # We assume the GT mask has a slightly different name\n","    # e.g., 'Rep1_0%_mask.png' -> 'Rep1_0%.png'\n","    # --- ADJUST THIS LOGIC IF YOURS IS DIFFERENT ---\n","    gt_filename = filename.replace(\".jpg\", \"_mask.png\") # Adjust as needed\n","    gt_path = os.path.join(GT_MASK_DIR, gt_filename)\n","\n","    # --- Load Masks ---\n","    # Load ground truth mask\n","    if not os.path.exists(gt_path):\n","        print(f\"Warning: Missing GT for {filename}, skipping.\")\n","        continue\n","\n","    gt_mask = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n","    # Load predicted mask\n","    pred_mask = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n","    if gt_mask is not None:\n","        gt_mask[gt_mask >= NUM_CLASSES] = 0\n","    if pred_mask is not None:\n","        pred_mask[pred_mask >= NUM_CLASSES] = 0\n","    # --- Sanity Check ---\n","    if gt_mask.shape != pred_mask.shape:\n","        print(f\"Warning: Shape mismatch for {filename}, skipping.\")\n","        print(f\"  GT Shape: {gt_mask.shape}, Pred Shape: {pred_mask.shape}\")\n","        continue\n","\n","    # --- Convert to One-Hot Format ---\n","    # The metrics functions expect (Batch, Classes, H, W)\n","    gt_onehot = to_one_hot(gt_mask, NUM_CLASSES).unsqueeze(0) # (1, C, H, W)\n","    pred_onehot = to_one_hot(pred_mask, NUM_CLASSES).unsqueeze(0) # (1, C, H, W)\n","\n","    # --- Calculate Metrics ---\n","    # These functions are class-aware and will return a score for each\n","    # class. We include background to keep indices consistent.\n","\n","    # Dice and IOU (higher is better, 0-1)\n","    # Returns a tensor of shape (1, C)\n","    dice_scores = compute_dice(pred_onehot, gt_onehot, include_background=True)\n","    iou_scores = compute_iou(pred_onehot, gt_onehot, include_background=True)\n","\n","    # Hausdorff Distance 95th Percentile (lower is better, in pixels)\n","    # HD95 is more robust to outliers than the standard Hausdorff.\n","    hd95_scores = compute_hausdorff_distance(\n","        pred_onehot,\n","        gt_onehot,\n","        include_background=True,\n","        percentile=95\n","    )\n","\n","    # --- Store Results ---\n","    # Store per-class metrics in a dictionary\n","    file_metrics = {'filename': filename}\n","    for i in range(NUM_CLASSES):\n","        class_name = CLASS_MAP[i]\n","\n","        # .item() converts the tensor value to a plain Python number\n","        file_metrics[f\"{class_name}_Dice\"] = dice_scores[0, i].item()\n","        file_metrics[f\"{class_name}_IOU\"] = iou_scores[0, i].item()\n","\n","        # HD is NaN if a class is missing from both pred and GT.\n","        # We'll store it as is.\n","        file_metrics[f\"{class_name}_HD95\"] = hd95_scores[0, i].item()\n","\n","    results_list.append(file_metrics)\n","\n","print(\"Analysis complete.\")\n","\n","# =================================================================================\n","# 4. REPORTING (The \"Deliverable\")\n","# =================================================================================\n","\n","# Convert the list of results into a pandas DataFrame\n","df = pd.DataFrame(results_list)\n","df.set_index('filename', inplace=True)\n","\n","# --- 1. Show a sample of the full results table ---\n","print(\"\\n--- Full Results Table (Sample) ---\")\n","print(df.head())\n","\n","# --- 2. Show the overall average statistics ---\n","# .mean() will automatically (and correctly) ignore NaNs\n","overall_stats = df.mean()\n","\n","print(\"\\n\\n--- Overall Average Statistics (Test Set) ---\")\n","print(\"This is the main result. We will use this table for our discussion.\")\n","\n","# Reshape the data for a cleaner summary table\n","summary_data = []\n","for i in range(NUM_CLASSES):\n","    class_name = CLASS_MAP[i]\n","    summary_data.append({\n","        \"Class\": class_name,\n","        \"Dice (↑)\": overall_stats.get(f\"{class_name}_Dice\"),\n","        \"IOU (↑)\": overall_stats.get(f\"{class_name}_IOU\"),\n","        \"HD95 (↓)\": overall_stats.get(f\"{class_name}_HD95\"),\n","    })\n","\n","summary_df = pd.DataFrame(summary_data)\n","print(summary_df.to_markdown(index=False, floatfmt=\".4f\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CI_ReTvKoPqh","executionInfo":{"status":"ok","timestamp":1763229925239,"user_tz":360,"elapsed":23916,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}},"outputId":"f7d28e3a-c164-443a-b48b-8dcc2ebb626c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting analysis...\n","GT Directory: /content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/masks/\n","Pred Directory: /content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/outputs/Unsqueezed/pred_masks/test/\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/52 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.metrics.utils get_mask_edges:always_return_as_numpy: Argument `always_return_as_numpy` has been deprecated since version 1.5.0. It will be removed in version 1.7.0. The option is removed and the return type will always be equal to the input type.\n","  warn_deprecated(argname, msg, warning_category)\n"," 15%|█▌        | 8/52 [00:06<00:33,  1.33it/s]/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 4 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n"," 83%|████████▎ | 43/52 [00:16<00:02,  3.44it/s]/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 1 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","100%|██████████| 52/52 [00:19<00:00,  2.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Analysis complete.\n","\n","--- Full Results Table (Sample) ---\n","                                              Background_Dice  Background_IOU  \\\n","filename                                                                        \n","Rep1_0%Sucrose_gaut10-3gaut11-3+_19.jpg              0.984967        0.970380   \n","Rep1_0%Sucrose_gaut3-1gaut11-3_29.jpg                0.991010        0.982180   \n","Rep2_0.5%Sucrose_gaut10-3_8.jpg                      0.987785        0.975866   \n","Rep2_0.5%Sucrose_gaut10-3gaut11-3_5.jpg              0.990434        0.981049   \n","Rep1_0%Sucrose_gaut3-1gaut10-3gaut11-3_8.jpg         0.995549        0.991138   \n","\n","                                              Background_HD95  Stem_Dice  \\\n","filename                                                                   \n","Rep1_0%Sucrose_gaut10-3gaut11-3+_19.jpg              7.565374   0.093120   \n","Rep1_0%Sucrose_gaut3-1gaut11-3_29.jpg                5.385165   0.097307   \n","Rep2_0.5%Sucrose_gaut10-3_8.jpg                      7.364026   0.269670   \n","Rep2_0.5%Sucrose_gaut10-3gaut11-3_5.jpg              5.830952   0.254500   \n","Rep1_0%Sucrose_gaut3-1gaut10-3gaut11-3_8.jpg         4.472136   0.233942   \n","\n","                                              Stem_IOU   Stem_HD95  Leaf_Dice  \\\n","filename                                                                        \n","Rep1_0%Sucrose_gaut10-3gaut11-3+_19.jpg       0.048834  421.858978   0.499195   \n","Rep1_0%Sucrose_gaut3-1gaut11-3_29.jpg         0.051142  375.123901   0.515057   \n","Rep2_0.5%Sucrose_gaut10-3_8.jpg               0.155849  352.440826   0.477699   \n","Rep2_0.5%Sucrose_gaut10-3gaut11-3_5.jpg       0.145803  327.384796   0.456095   \n","Rep1_0%Sucrose_gaut3-1gaut10-3gaut11-3_8.jpg  0.132466  119.320793   0.477907   \n","\n","                                              Leaf_IOU   Leaf_HD95  Root_Dice  \\\n","filename                                                                        \n","Rep1_0%Sucrose_gaut10-3gaut11-3+_19.jpg       0.332618  431.589783   0.673901   \n","Rep1_0%Sucrose_gaut3-1gaut11-3_29.jpg         0.346853  363.560181   0.675799   \n","Rep2_0.5%Sucrose_gaut10-3_8.jpg               0.313801  374.388000   0.681775   \n","Rep2_0.5%Sucrose_gaut10-3gaut11-3_5.jpg       0.295416  325.701996   0.730193   \n","Rep1_0%Sucrose_gaut3-1gaut10-3gaut11-3_8.jpg  0.313980  110.795975   0.339369   \n","\n","                                              Root_IOU  Root_HD95  Seed_Dice  \\\n","filename                                                                       \n","Rep1_0%Sucrose_gaut10-3gaut11-3+_19.jpg       0.508183  25.286345   0.096535   \n","Rep1_0%Sucrose_gaut3-1gaut11-3_29.jpg         0.510345  16.280117   0.226000   \n","Rep2_0.5%Sucrose_gaut10-3_8.jpg               0.517192  53.945644   0.005682   \n","Rep2_0.5%Sucrose_gaut10-3gaut11-3_5.jpg       0.575042  52.992435   0.234613   \n","Rep1_0%Sucrose_gaut3-1gaut10-3gaut11-3_8.jpg  0.204361  34.412159   0.304636   \n","\n","                                              Seed_IOU   Seed_HD95  \n","filename                                                            \n","Rep1_0%Sucrose_gaut10-3gaut11-3+_19.jpg       0.050715  425.690033  \n","Rep1_0%Sucrose_gaut3-1gaut11-3_29.jpg         0.127396  353.249146  \n","Rep2_0.5%Sucrose_gaut10-3_8.jpg               0.002849  378.400421  \n","Rep2_0.5%Sucrose_gaut10-3gaut11-3_5.jpg       0.132896  350.923401  \n","Rep1_0%Sucrose_gaut3-1gaut10-3gaut11-3_8.jpg  0.179688  104.284981  \n","\n","\n","--- Overall Average Statistics (Test Set) ---\n","This is the main result. We will use this table for our discussion.\n","| Class      |   Dice (↑) |   IOU (↑) |   HD95 (↓) |\n","|:-----------|-----------:|----------:|-----------:|\n","| Background |     0.9922 |    0.9846 |     8.3472 |\n","| Stem       |     0.1707 |    0.0956 |   286.0556 |\n","| Leaf       |     0.4640 |    0.3061 |   285.5463 |\n","| Root       |     0.6058 |    0.4508 |    39.8493 |\n","| Seed       |     0.1712 |    0.1005 |   289.1759 |\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}