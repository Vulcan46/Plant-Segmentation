{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","mount_file_id":"18MhwRAmHV9QHbJCQJV9tHh-nityh38h0","authorship_tag":"ABX9TyOrVn206Tww6TCzi/v4RgCH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f2f8061b6b5a431e9762ca608f15652e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2b9064ef1ed4abb9a746eb3c6e0b104","IPY_MODEL_bb7f784608ed4380910f5af7e8021958","IPY_MODEL_650a3e34ed764c2d910a08101d054d01"],"layout":"IPY_MODEL_ae5e6299795f4d82acf04bb2bc7bba12"}},"c2b9064ef1ed4abb9a746eb3c6e0b104":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9b883a1a76a461181b37cf53b98e7ca","placeholder":"​","style":"IPY_MODEL_5fda302b511d4ced89666adbe3bab2ae","value":"config.json: 100%"}},"bb7f784608ed4380910f5af7e8021958":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_628842726a8d4e69816cb90c98931e8d","max":156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_397047953dbd4fb6b9b45567277f93d5","value":156}},"650a3e34ed764c2d910a08101d054d01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60e015391cae48e281bcc2dbe4584f60","placeholder":"​","style":"IPY_MODEL_58c3183659b04f44bb41106b9905dae5","value":" 156/156 [00:00&lt;00:00, 19.3kB/s]"}},"ae5e6299795f4d82acf04bb2bc7bba12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9b883a1a76a461181b37cf53b98e7ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fda302b511d4ced89666adbe3bab2ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"628842726a8d4e69816cb90c98931e8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"397047953dbd4fb6b9b45567277f93d5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60e015391cae48e281bcc2dbe4584f60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58c3183659b04f44bb41106b9905dae5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88ff150d676d4ddc8eadb4963d36fc3f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_486c2a8fa0f34cc1bfd83b31dbaa29b1","IPY_MODEL_50533a52383143e8a4c36f84e98c4ca8","IPY_MODEL_5e0fd148600046a5929341e254750523"],"layout":"IPY_MODEL_db74d89ae91e471c859fb095eccb446d"}},"486c2a8fa0f34cc1bfd83b31dbaa29b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1bcaf2ad16b45868922763f398fb318","placeholder":"​","style":"IPY_MODEL_016edc13d74942c882b9985b3f4b6919","value":"model.safetensors: 100%"}},"50533a52383143e8a4c36f84e98c4ca8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfff18b5de5345c586ac375454c8ca8a","max":87275112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_313718a86db6496084208f27bf40ef1f","value":87275112}},"5e0fd148600046a5929341e254750523":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc31232d5e3a483a91e580a1d4e76f53","placeholder":"​","style":"IPY_MODEL_c2df49fea6604392960e875c3b928bbc","value":" 87.3M/87.3M [00:01&lt;00:00, 14.0MB/s]"}},"db74d89ae91e471c859fb095eccb446d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1bcaf2ad16b45868922763f398fb318":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"016edc13d74942c882b9985b3f4b6919":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfff18b5de5345c586ac375454c8ca8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"313718a86db6496084208f27bf40ef1f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc31232d5e3a483a91e580a1d4e76f53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2df49fea6604392960e875c3b928bbc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"vuR3_Zxgm8CL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765559967071,"user_tz":360,"elapsed":37015,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}},"outputId":"437bb618-674d-40dc-e28e-e87d862df636"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]},{"output_type":"stream","name":"stderr","text":["<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"]}],"source":["# =================================================================================\n","# 0. SETUP AND IMPORTS\n","# =================================================================================\n","# Force upgrade to fix potential torch attribute errors\n","!pip install -q segmentation-models-pytorch albumentations monai\n","\n","import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from PIL import Image\n","from tqdm import tqdm\n","import random\n","\n","from monai.losses import DiceLoss\n","\n"]},{"cell_type":"code","source":["# =================================================================================\n","# 1. CONFIGURATION (V5: SPLIT LEAF)\n","# =================================================================================\n","class Config:\n","    # -- Base Paths --\n","    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/\"\n","\n","    # Path to ORIGINAL images (JPGs)\n","    # We use the combined_data images folder\n","    IMAGE_DIR = os.path.join(BASE_PATH, \"images/\")\n","\n","    # Path to NEW CSV MASKS\n","    # Update this to where you uploaded the \"split_masks\" folder\n","    MASK_CSV_DIR = os.path.join(BASE_PATH, \"augmented masks v3/split_masks\")\n","    # QC Report & Splits\n","    QC_REPORT_CSV = os.path.join(BASE_PATH, \"missing_classes_from_mask.csv\")\n","    SPLIT_CSV = os.path.join(BASE_PATH, \"dataset_split.csv\")\n","\n","    # -- Output Paths --\n","    OUTPUT_DIR = os.path.join(BASE_PATH, \"outputs_v5_split_leaf_final\")\n","    OUTPUT_MASK_DIR = os.path.join(OUTPUT_DIR, \"pred_masks\")\n","    COLOR_MASK_DIR = os.path.join(OUTPUT_DIR, \"color_masks\")\n","\n","    # -- Model Hyperparameters --\n","    ARCHITECTURE = 'unetplusplus'\n","    ENCODER = 'resnet34'\n","    ENCODER_WEIGHTS = 'imagenet'\n","    LEARNING_RATE = 1e-4\n","    OPTIMIZER = 'AdamW'\n","\n","    # -- Training Settings --\n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    BATCH_SIZE = 4\n","    NUM_EPOCHS = 40\n","\n","    IMAGE_HEIGHT = 512\n","    IMAGE_WIDTH = 512\n","\n","    # 0=Bg, 1=Root, 2=Unused, 3=Stem, 4=Seed, 5=Left, 6=Right\n","    NUM_CLASSES = 7\n","\n","    # -- LOSS CONFIGURATION --\n","    # Weights mapped to the new indices\n","    CLASS_WEIGHTS = torch.tensor([\n","        1.0,  # 0: Background\n","        10.0,  # 1: Root\n","        0.0,  # 2: UNUSED (Weight 0 so model doesn't care)\n","        10.0,  # 3: Stem\n","        15.0, # 4: Seed\n","        7.0,  # 5: Left Leaf\n","        7.0   # 6: Right Leaf\n","    ], device=DEVICE)\n","\n","    # -- Visualization --\n","    COLOR_MAP = {\n","        0: (0, 0, 0),        # Bg\n","        1: (255, 255, 0),    # Root (Yellow)\n","        2: (0, 0, 0),        # Unused\n","        3: (139, 69, 19),    # Stem (Brown)\n","        4: (255, 0, 0),      # Seed (Red)\n","        5: (0, 255, 0),      # Left Leaf (Green)\n","        6: (0, 128, 0),      # Right Leaf (Dark Green)\n","    }\n","\n","os.makedirs(Config.OUTPUT_MASK_DIR, exist_ok=True)\n","os.makedirs(Config.COLOR_MASK_DIR, exist_ok=True)\n","\n"],"metadata":{"id":"4YYxF11ZqT3g","executionInfo":{"status":"ok","timestamp":1765560180777,"user_tz":360,"elapsed":11,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def get_splits_v5(image_dir, mask_dir, split_csv_path, qc_csv_path):\n","    print(\"--- Configuring V5 Data Splits ---\")\n","    if os.path.exists(qc_csv_path):\n","        df_qc = pd.read_csv(qc_csv_path)\n","        # Find rows where Mask_correct is explicitly FALSE\n","        bad_rows = df_qc[df_qc['Mask_correct'].astype(str).str.upper() == 'FALSE']\n","        excluded_mask_filenames = set(bad_rows['filename'].tolist())\n","        print(f\"QC Report loaded. Found {len(excluded_mask_filenames)} bad masks to exclude.\")\n","    else:\n","        print(\"Warning: QC CSV not found. No masks will be blacklisted.\")\n","\n","    # Load Split Map\n","    split_map = {}\n","    if os.path.exists(split_csv_path):\n","        try:\n","            df_split = pd.read_csv(split_csv_path)\n","        except:\n","            df_split = pd.read_excel(split_csv_path.replace('.csv', '.xlsx'))\n","        for idx, row in df_split.iterrows():\n","            split_map[row['img_name']] = row['set'].lower().strip()\n","\n","    all_images = sorted([f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))])\n","\n","    train_files, val_files, test_files = [], [], []\n","\n","    for img_name in all_images:\n","      # Construct mask name for QC check\n","        mask_name = os.path.splitext(img_name)[0] + \"_mask.png\"\n","\n","        # --- CHECK 1: Is it mislabeled? ---\n","        if mask_name in excluded_mask_filenames:\n","            continue # SKIP THIS IMAGE COMPLETELY\n","        # Check if Mask file exists (CSV or TSV)\n","        base_name = os.path.splitext(img_name)[0]\n","        possible_names = [\n","            base_name + \".csv\", base_name + \"_mask.csv\",\n","            base_name + \".tsv\", base_name + \"_mask.tsv\"\n","        ]\n","\n","        mask_path = None\n","        for name in possible_names:\n","            p = os.path.join(mask_dir, name)\n","            if os.path.exists(p):\n","                mask_path = p\n","                break\n","\n","        if mask_path is None:\n","            continue\n","\n","        # Split assignment\n","        if img_name in split_map:\n","            assigned_set = split_map[img_name]\n","            if assigned_set == 'train': train_files.append(img_name)\n","            elif assigned_set == 'val': val_files.append(img_name)\n","            elif assigned_set == 'test': test_files.append(img_name)\n","        else:\n","            train_files.append(img_name) # Synthetic/Extra to Train\n","\n","    print(f\"Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}\")\n","    return train_files, val_files, test_files\n"],"metadata":{"id":"Q_NJS97hqT2X","executionInfo":{"status":"ok","timestamp":1765560259116,"user_tz":360,"elapsed":24,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class RobustMaskDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, image_filenames, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.images = image_filenames\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        img_name = self.images[index]\n","        img_path = os.path.join(self.image_dir, img_name)\n","\n","        # Load Image\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","\n","        # Load Mask (Try CSV/TSV)\n","        base_name = os.path.splitext(img_name)[0]\n","        possible_names = [\n","            base_name + \".csv\", base_name + \"_mask.csv\",\n","            base_name + \".tsv\", base_name + \"_mask.tsv\"\n","        ]\n","        mask_path = None\n","        for name in possible_names:\n","            p = os.path.join(self.mask_dir, name)\n","            if os.path.exists(p):\n","                mask_path = p\n","                break\n","\n","        # --- ROBUST LOADING LOGIC ---\n","        if mask_path is None:\n","            mask = np.zeros(image.shape[:2], dtype=np.float32)\n","        else:\n","            try:\n","                # Determine delimiter based on extension or sniffing\n","                delimiter = ',' if mask_path.endswith('.csv') else '\\t'\n","\n","                # If reading fails with delimiter, try whitespace ' '\n","                try:\n","                    df = pd.read_csv(mask_path, header=None, sep=delimiter)\n","                    # Check if it loaded as one column (parsing error)\n","                    if df.shape[1] == 1:\n","                         # Fallback to space separated\n","                         df = pd.read_csv(mask_path, header=None, sep='\\s+')\n","                except:\n","                    # Final fallback: space/tab/arbitrary whitespace\n","                    df = pd.read_csv(mask_path, header=None, sep='\\s+')\n","\n","                mask = df.values.astype(np.float32)\n","            except Exception as e:\n","                print(f\"Error reading mask {mask_path}: {e}\")\n","                mask = np.zeros(image.shape[:2], dtype=np.float32)\n","\n","        # --- SMART PADDING LOGIC ---\n","        # The mask is the \"Ground Truth\" dimension.\n","        # If image is smaller, pad it to match mask.\n","        h_img, w_img = image.shape[:2]\n","        h_mask, w_mask = mask.shape[:2]\n","\n","        if (h_img != h_mask) or (w_img != w_mask):\n","            # Create a padded canvas for the image\n","            padded_image = np.zeros((h_mask, w_mask, 3), dtype=np.uint8)\n","\n","            # Paste image at top-left (0,0)\n","            # Clip dimensions to avoid errors if image is somehow larger\n","            h_paste = min(h_img, h_mask)\n","            w_paste = min(w_img, w_mask)\n","\n","            padded_image[:h_paste, :w_paste, :] = image[:h_paste, :w_paste, :]\n","            image = padded_image\n","\n","            # NOTE: We assume top-left alignment based on typical canvas generation.\n","\n","        original_height, original_width = h_mask, w_mask\n","\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        return image, mask.long(), (original_height, original_width)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yik-tQ6aqm_4","executionInfo":{"status":"ok","timestamp":1765559970229,"user_tz":360,"elapsed":33,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}},"outputId":"dec9a407-d227-4744-aee6-a7ae2682fdb9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:45: SyntaxWarning: invalid escape sequence '\\s'\n","<>:48: SyntaxWarning: invalid escape sequence '\\s'\n","<>:45: SyntaxWarning: invalid escape sequence '\\s'\n","<>:48: SyntaxWarning: invalid escape sequence '\\s'\n","/tmp/ipython-input-409968435.py:45: SyntaxWarning: invalid escape sequence '\\s'\n","  df = pd.read_csv(mask_path, header=None, sep='\\s+')\n","/tmp/ipython-input-409968435.py:48: SyntaxWarning: invalid escape sequence '\\s'\n","  df = pd.read_csv(mask_path, header=None, sep='\\s+')\n"]}]},{"cell_type":"code","source":["class WeightedDiceLoss(nn.Module):\n","    def __init__(self, class_weights):\n","        super().__init__()\n","        self.class_weights = class_weights\n","        self.dice_loss = DiceLoss(softmax=True, to_onehot_y=True, include_background=True, reduction='none')\n","\n","    def forward(self, preds, targets):\n","        targets = targets.unsqueeze(1)\n","        loss_per_class = self.dice_loss(preds, targets)\n","        weighted_loss = loss_per_class * self.class_weights\n","        return weighted_loss.mean()\n","\n","def mask_to_rgb(mask_tensor, color_map):\n","    mask = mask_tensor.cpu().numpy().squeeze()\n","    rgb_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n","    for class_idx, color in color_map.items():\n","        rgb_mask[mask == class_idx] = color\n","    return Image.fromarray(rgb_mask)\n","\n","def train_fn(loader, model, optimizer, loss_fn, scaler):\n","    loop = tqdm(loader, desc=\"Training\")\n","    total_loss = 0\n","    for batch_idx, (data, targets, _) in enumerate(loop):\n","        data = data.to(device=Config.DEVICE)\n","        targets = targets.to(device=Config.DEVICE)\n","        with torch.amp.autocast('cuda'):\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total_loss += loss.item()\n","        loop.set_postfix(loss=loss.item())\n","    return total_loss / len(loader)\n","\n","def eval_fn(loader, model, loss_fn):\n","    model.eval()\n","    total_loss = 0\n","    loop = tqdm(loader, desc=\"Validation\")\n","    with torch.no_grad():\n","        for data, targets, _ in loop:\n","            data = data.to(device=Config.DEVICE)\n","            targets = targets.to(device=Config.DEVICE)\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","            total_loss += loss.item()\n","            loop.set_postfix(val_loss=loss.item())\n","    model.train()\n","    return total_loss / len(loader)\n","\n","def save_predictions_fn(loader, model, folder_basename=\"\"):\n","    print(f\"\\n--- Saving predictions for {folder_basename} set ---\")\n","    model.eval()\n","    output_mask_dir = os.path.join(Config.OUTPUT_MASK_DIR, folder_basename)\n","    color_mask_dir = os.path.join(Config.COLOR_MASK_DIR, folder_basename)\n","    os.makedirs(output_mask_dir, exist_ok=True)\n","    os.makedirs(color_mask_dir, exist_ok=True)\n","\n","    for idx in tqdm(range(len(loader.dataset))):\n","        img_tensor, _, (original_h, original_w) = loader.dataset[idx]\n","        with torch.no_grad():\n","            img_tensor = img_tensor.to(Config.DEVICE).unsqueeze(0)\n","            preds = model(img_tensor)\n","            final_mask_tensor = torch.argmax(preds, dim=1).squeeze(0)\n","\n","        pred_mask_np = final_mask_tensor.cpu().numpy().astype(np.uint8)\n","        resized_mask = cv2.resize(pred_mask_np, (original_w, original_h), interpolation=cv2.INTER_NEAREST)\n","\n","        original_filename = loader.dataset.images[idx]\n","        name_only = os.path.splitext(original_filename)[0]\n","\n","        Image.fromarray(resized_mask).save(os.path.join(output_mask_dir, name_only + \"_mask.png\"))\n","        mask_to_rgb(torch.from_numpy(resized_mask), Config.COLOR_MAP).save(os.path.join(color_mask_dir, name_only + \"_mask.png\"))\n","    model.train()"],"metadata":{"id":"sZYgrcnxqtY4","executionInfo":{"status":"ok","timestamp":1765559970259,"user_tz":360,"elapsed":26,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Augmentations\n","train_transform = A.Compose([\n","    A.Resize(height=Config.IMAGE_HEIGHT, width=Config.IMAGE_WIDTH),\n","    A.Rotate(limit=35, p=0.5),\n","    A.HorizontalFlip(p=0.5),\n","    A.VerticalFlip(p=0.5),\n","    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n","    ToTensorV2(),\n","])\n","\n","val_transform = A.Compose([\n","    A.Resize(height=Config.IMAGE_HEIGHT, width=Config.IMAGE_WIDTH),\n","    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n","    ToTensorV2(),\n","])"],"metadata":{"id":"wzDi2MQifD04","executionInfo":{"status":"ok","timestamp":1765559970364,"user_tz":360,"elapsed":103,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def main():\n","    print(f\"Using device: {Config.DEVICE}\")\n","\n","    # 1. Splits\n","    train_files, val_files, test_files = get_splits_v5(Config.IMAGE_DIR, Config.MASK_CSV_DIR, Config.SPLIT_CSV, Config.QC_REPORT_CSV)\n","\n","    if len(train_files) == 0:\n","        print(\"Error: No matching mask files found! Check MASK_CSV_DIR path.\")\n","        return\n","\n","    # 2. Loaders\n","    train_dataset = RobustMaskDataset(Config.IMAGE_DIR, Config.MASK_CSV_DIR, train_files, train_transform)\n","    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n","    val_dataset = RobustMaskDataset(Config.IMAGE_DIR, Config.MASK_CSV_DIR, val_files, val_transform)\n","    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n","    test_dataset = RobustMaskDataset(Config.IMAGE_DIR, Config.MASK_CSV_DIR, test_files, val_transform)\n","    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","    # 3. Model\n","    model = smp.create_model(\n","        arch=Config.ARCHITECTURE,\n","        encoder_name=Config.ENCODER,\n","        encoder_weights=Config.ENCODER_WEIGHTS,\n","        in_channels=3,\n","        classes=Config.NUM_CLASSES\n","    ).to(Config.DEVICE)\n","\n","    loss_fn = WeightedDiceLoss(class_weights=Config.CLASS_WEIGHTS)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n","    scaler = torch.amp.GradScaler('cuda')\n","    best_val_loss = float('inf')\n","\n","    # 4. Train\n","    for epoch in range(Config.NUM_EPOCHS):\n","        print(f\"\\n--- Epoch {epoch+1}/{Config.NUM_EPOCHS} ---\")\n","        train_loss = train_fn(train_loader, model, optimizer, loss_fn, scaler)\n","        val_loss = eval_fn(val_loader, model, loss_fn)\n","        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), os.path.join(Config.BASE_PATH, \"best_model_v5_robust.pth\"))\n","            print(\"=> Saved new best model\")\n","\n","    # 5. Save\n","    print(\"\\n--- Testing Best Model ---\")\n","    model.load_state_dict(torch.load(os.path.join(Config.BASE_PATH, \"best_model_v5_robust.pth\")))\n","    save_predictions_fn(test_loader, model, folder_basename=\"test_set\")\n","    print(\"--- V5 Robust Training Complete ---\")\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f2f8061b6b5a431e9762ca608f15652e","c2b9064ef1ed4abb9a746eb3c6e0b104","bb7f784608ed4380910f5af7e8021958","650a3e34ed764c2d910a08101d054d01","ae5e6299795f4d82acf04bb2bc7bba12","c9b883a1a76a461181b37cf53b98e7ca","5fda302b511d4ced89666adbe3bab2ae","628842726a8d4e69816cb90c98931e8d","397047953dbd4fb6b9b45567277f93d5","60e015391cae48e281bcc2dbe4584f60","58c3183659b04f44bb41106b9905dae5","88ff150d676d4ddc8eadb4963d36fc3f","486c2a8fa0f34cc1bfd83b31dbaa29b1","50533a52383143e8a4c36f84e98c4ca8","5e0fd148600046a5929341e254750523","db74d89ae91e471c859fb095eccb446d","f1bcaf2ad16b45868922763f398fb318","016edc13d74942c882b9985b3f4b6919","bfff18b5de5345c586ac375454c8ca8a","313718a86db6496084208f27bf40ef1f","dc31232d5e3a483a91e580a1d4e76f53","c2df49fea6604392960e875c3b928bbc"]},"id":"o-k8s2N7qxp4","executionInfo":{"status":"ok","timestamp":1765562034804,"user_tz":360,"elapsed":1757833,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}},"outputId":"4941580c-42ca-4c31-b831-97b3eaef97f1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","--- Configuring V5 Data Splits ---\n","QC Report loaded. Found 32 bad masks to exclude.\n","Train: 389 | Val: 47 | Test: 47\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2f8061b6b5a431e9762ca608f15652e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88ff150d676d4ddc8eadb4963d36fc3f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- Epoch 1/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [05:56<00:00,  3.64s/it, loss=6.33]\n","Validation: 100%|██████████| 12/12 [00:42<00:00,  3.57s/it, val_loss=6.24]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 6.5468 | Val Loss: 6.1245\n","=> Saved new best model\n","\n","--- Epoch 2/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.20it/s, loss=6.01]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.27it/s, val_loss=5.8]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 6.0280 | Val Loss: 5.5089\n","=> Saved new best model\n","\n","--- Epoch 3/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.24it/s, loss=4.96]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.33it/s, val_loss=5.14]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 5.2069 | Val Loss: 4.5043\n","=> Saved new best model\n","\n","--- Epoch 4/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.21it/s, loss=4.46]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.29it/s, val_loss=4.5]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 4.1032 | Val Loss: 3.6643\n","=> Saved new best model\n","\n","--- Epoch 5/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.21it/s, loss=4.44]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.28it/s, val_loss=3.87]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 3.4077 | Val Loss: 3.2105\n","=> Saved new best model\n","\n","--- Epoch 6/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.20it/s, loss=3.16]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.27it/s, val_loss=3.61]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 3.0859 | Val Loss: 3.2387\n","\n","--- Epoch 7/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.47it/s, loss=2.73]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.15it/s, val_loss=3.07]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.9322 | Val Loss: 2.7952\n","=> Saved new best model\n","\n","--- Epoch 8/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.26it/s, loss=2.35]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.28it/s, val_loss=2.89]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.7820 | Val Loss: 2.6280\n","=> Saved new best model\n","\n","--- Epoch 9/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.23it/s, loss=2.71]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.30it/s, val_loss=3.04]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.7004 | Val Loss: 2.6428\n","\n","--- Epoch 10/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.49it/s, loss=2.98]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.39it/s, val_loss=3.28]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.6307 | Val Loss: 2.7281\n","\n","--- Epoch 11/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=4.53]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.26it/s, val_loss=2.62]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.5961 | Val Loss: 2.5999\n","=> Saved new best model\n","\n","--- Epoch 12/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.24it/s, loss=2.58]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.35it/s, val_loss=2.73]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.5364 | Val Loss: 2.4950\n","=> Saved new best model\n","\n","--- Epoch 13/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.26it/s, loss=2.03]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.40it/s, val_loss=2.66]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.4967 | Val Loss: 2.5709\n","\n","--- Epoch 14/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.49it/s, loss=2.43]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.31it/s, val_loss=2.91]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.4554 | Val Loss: 2.5745\n","\n","--- Epoch 15/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.47it/s, loss=2.22]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.43it/s, val_loss=3.29]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.5569 | Val Loss: 2.6843\n","\n","--- Epoch 16/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.49it/s, loss=4.67]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.31it/s, val_loss=3.01]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.4695 | Val Loss: 2.5671\n","\n","--- Epoch 17/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.46it/s, loss=2.25]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.41it/s, val_loss=2.9]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.4791 | Val Loss: 2.5465\n","\n","--- Epoch 18/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=4.28]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.30it/s, val_loss=2.57]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.4510 | Val Loss: 2.4173\n","=> Saved new best model\n","\n","--- Epoch 19/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.21it/s, loss=2.77]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.29it/s, val_loss=2.81]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.4440 | Val Loss: 2.5190\n","\n","--- Epoch 20/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=2.71]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.15it/s, val_loss=2.43]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3899 | Val Loss: 2.4160\n","=> Saved new best model\n","\n","--- Epoch 21/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.21it/s, loss=2.96]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.24it/s, val_loss=2.51]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3914 | Val Loss: 2.5609\n","\n","--- Epoch 22/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.46it/s, loss=2.12]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.34it/s, val_loss=2.46]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3954 | Val Loss: 2.3945\n","=> Saved new best model\n","\n","--- Epoch 23/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.25it/s, loss=2.9]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.26it/s, val_loss=2.46]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3351 | Val Loss: 2.4431\n","\n","--- Epoch 24/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=2.26]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.36it/s, val_loss=2.36]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3194 | Val Loss: 2.4007\n","\n","--- Epoch 25/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.47it/s, loss=3.22]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.28it/s, val_loss=3.57]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3727 | Val Loss: 2.6869\n","\n","--- Epoch 26/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=2.58]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.31it/s, val_loss=2.42]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3914 | Val Loss: 2.4167\n","\n","--- Epoch 27/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.50it/s, loss=2.08]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.28it/s, val_loss=2.57]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3474 | Val Loss: 2.3602\n","=> Saved new best model\n","\n","--- Epoch 28/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:29<00:00,  3.27it/s, loss=3.72]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.26it/s, val_loss=2.4]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3445 | Val Loss: 2.3913\n","\n","--- Epoch 29/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=1.7]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.22it/s, val_loss=2.52]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3036 | Val Loss: 2.3028\n","=> Saved new best model\n","\n","--- Epoch 30/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.25it/s, loss=1.71]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.32it/s, val_loss=2.41]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2879 | Val Loss: 2.3330\n","\n","--- Epoch 31/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=1.73]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.36it/s, val_loss=2.35]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2743 | Val Loss: 2.4148\n","\n","--- Epoch 32/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.49it/s, loss=3.71]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.25it/s, val_loss=2.43]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2684 | Val Loss: 2.3150\n","\n","--- Epoch 33/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.47it/s, loss=2.09]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.26it/s, val_loss=2.45]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2504 | Val Loss: 2.3641\n","\n","--- Epoch 34/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.47it/s, loss=1.91]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.25it/s, val_loss=2.66]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.3060 | Val Loss: 2.3086\n","\n","--- Epoch 35/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=2.63]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.29it/s, val_loss=2.42]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2540 | Val Loss: 2.3418\n","\n","--- Epoch 36/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=1.9]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.31it/s, val_loss=2.38]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2683 | Val Loss: 2.3043\n","\n","--- Epoch 37/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=1.91]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.35it/s, val_loss=2.48]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2143 | Val Loss: 2.3024\n","=> Saved new best model\n","\n","--- Epoch 38/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:30<00:00,  3.23it/s, loss=1.82]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.28it/s, val_loss=2.53]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2464 | Val Loss: 2.3090\n","\n","--- Epoch 39/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=2.14]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.24it/s, val_loss=2.38]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2214 | Val Loss: 2.3393\n","\n","--- Epoch 40/40 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [00:28<00:00,  3.48it/s, loss=1.83]\n","Validation: 100%|██████████| 12/12 [00:02<00:00,  5.17it/s, val_loss=3.16]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 2.2396 | Val Loss: 2.5361\n","\n","--- Testing Best Model ---\n","\n","--- Saving predictions for test_set set ---\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 47/47 [01:57<00:00,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["--- V5 Robust Training Complete ---\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# =================================================================================\n","# 0. SETUP AND IMPORTS\n","# =================================================================================\n","!pip install -q monai pandas\n","\n","import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","from tqdm import tqdm\n","from monai.metrics import (\n","    compute_dice,\n","    compute_iou,\n","    compute_hausdorff_distance\n",")\n","\n","# =================================================================================\n","# 1. CONFIGURATION\n","# =================================================================================\n","# --- PATHS FOR V5 ---\n","# Ground Truth Masks (The NEW CSV/TSV masks folder)\n","GT_MASK_DIR = \"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/augmented masks v3/split_masks\"\n","\n","# Predicted Masks (From V5 Output)\n","PRED_MASK_DIR = \"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/outputs_v5_split_leaf_final/pred_masks/test_set/\"\n","\n","# --- CLASS MAP FOR V5 (7 Classes) ---\n","NUM_CLASSES = 7\n","CLASS_MAP = {\n","    0: \"Background\",\n","    1: \"Root\",\n","    2: \"Unused\",\n","    3: \"Stem\",\n","    4: \"Seed\",\n","    5: \"Left Leaf\",\n","    6: \"Right Leaf\"\n","}\n","\n","# =================================================================================\n","# 2. HELPER FUNCTIONS\n","# =================================================================================\n","def to_one_hot(mask, num_classes):\n","    \"\"\"Converts a (H, W) mask to (1, C, H, W) one-hot tensor.\"\"\"\n","    mask[mask >= num_classes] = 0 # Safety clip\n","    one_hot = np.eye(num_classes)[mask]\n","    one_hot = np.transpose(one_hot, (2, 0, 1))\n","    return torch.from_numpy(one_hot).unsqueeze(0)\n","\n","def load_gt_mask(filename):\n","    \"\"\"Robustly loads GT mask from CSV or TSV.\"\"\"\n","    base_name = os.path.splitext(filename)[0].replace(\"_mask\", \"\") # Remove _mask from prediction name\n","    possible_names = [base_name + \".csv\", base_name + \".tsv\", base_name + \"_mask.csv\", base_name + \"_mask.tsv\"]\n","\n","    mask_path = None\n","    for name in possible_names:\n","        p = os.path.join(GT_MASK_DIR, name)\n","        if os.path.exists(p):\n","            mask_path = p\n","            break\n","\n","    if mask_path is None:\n","        return None\n","\n","    try:\n","        delimiter = ',' if mask_path.endswith('.csv') else '\\t'\n","        try:\n","            df = pd.read_csv(mask_path, header=None, sep=delimiter)\n","            if df.shape[1] == 1:\n","                 df = pd.read_csv(mask_path, header=None, delim_whitespace=True)\n","        except:\n","            df = pd.read_csv(mask_path, header=None, delim_whitespace=True)\n","        return df.values.astype(np.int64)\n","    except:\n","        return None\n","\n","# =================================================================================\n","# 3. MAIN ANALYSIS LOOP\n","# =================================================================================\n","def run_analysis():\n","    print(\"Starting V5 Analysis...\")\n","    print(f\"Pred Directory: {PRED_MASK_DIR}\")\n","\n","    results_list = []\n","    pred_files = [f for f in os.listdir(PRED_MASK_DIR) if f.endswith('.png')]\n","\n","    if len(pred_files) == 0:\n","        print(\"❌ Error: No prediction files found! Check your PRED_MASK_DIR path.\")\n","        return\n","\n","    for filename in tqdm(pred_files):\n","        pred_path = os.path.join(PRED_MASK_DIR, filename)\n","\n","        # Load Prediction\n","        pred_mask = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n","\n","        # Load Ground Truth (Using robust loader)\n","        gt_mask = load_gt_mask(filename)\n","\n","        if gt_mask is None or pred_mask is None:\n","            continue\n","\n","        # Resize GT to match Pred (Using our Robust Training Logic: Pred is Image Size)\n","        # In training, we padded Image to match GT.\n","        # But prediction output was \"unsqueezed\" to original image size?\n","        # Wait, the V5 script output raw mask size (padded).\n","        # Let's ensure dimensions match by resizing GT to Pred if needed (or vice versa).\n","        # Safest bet: Resize GT to Pred size (Nearest Neighbor) to emulate \"Original Image Space\" evaluation\n","        if gt_mask.shape != pred_mask.shape:\n","             gt_mask = cv2.resize(gt_mask.astype(np.uint8), (pred_mask.shape[1], pred_mask.shape[0]), interpolation=cv2.INTER_NEAREST)\n","\n","        # Convert to One-Hot\n","        gt_onehot = to_one_hot(gt_mask, NUM_CLASSES)\n","        pred_onehot = to_one_hot(pred_mask, NUM_CLASSES)\n","\n","        # Metrics\n","        dice = compute_dice(pred_onehot, gt_onehot, include_background=True)\n","        iou = compute_iou(pred_onehot, gt_onehot, include_background=True)\n","        hd95 = compute_hausdorff_distance(pred_onehot, gt_onehot, include_background=True, percentile=95)\n","\n","        # Store\n","        file_metrics = {'filename': filename}\n","        for i in range(NUM_CLASSES):\n","            if i == 2: continue # Skip Unused Class\n","            c_name = CLASS_MAP[i]\n","            file_metrics[f\"{c_name}_Dice\"] = dice[0, i].item()\n","            file_metrics[f\"{c_name}_IOU\"] = iou[0, i].item()\n","            file_metrics[f\"{c_name}_HD95\"] = hd95[0, i].item()\n","\n","        results_list.append(file_metrics)\n","\n","    # Report\n","    if not results_list:\n","        print(\"No results generated.\")\n","        return\n","\n","    df = pd.DataFrame(results_list)\n","    overall_stats = df.mean(numeric_only=True)\n","\n","    print(\"\\n\\n--- V5 Overall Average Statistics (Test Set) ---\")\n","    summary_data = []\n","    for i in range(NUM_CLASSES):\n","        if i == 2: continue\n","        c_name = CLASS_MAP[i]\n","        summary_data.append({\n","            \"Class\": c_name,\n","            \"Dice (↑)\": overall_stats.get(f\"{c_name}_Dice\"),\n","            \"IOU (↑)\": overall_stats.get(f\"{c_name}_IOU\"),\n","            \"HD95 (↓)\": overall_stats.get(f\"{c_name}_HD95\"),\n","        })\n","\n","    summary_df = pd.DataFrame(summary_data)\n","    print(summary_df.to_markdown(index=False, floatfmt=\".4f\"))\n","\n","    summary_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/v5_results_summary.csv\", index=False)\n","    print(\"\\nSummary saved.\")\n","\n","if __name__ == \"__main__\":\n","    run_analysis()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0lpQ4RhafmxF","outputId":"d419f8dd-a373-4b9b-fa4f-fa8017c7d877"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting V5 Analysis...\n","Pred Directory: /content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/outputs_v5_split_leaf_final/pred_masks/test_set/\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/52 [00:00<?, ?it/s]/tmp/ipython-input-4239864296.py:70: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  df = pd.read_csv(mask_path, header=None, delim_whitespace=True)\n","/usr/local/lib/python3.12/dist-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.metrics.utils get_mask_edges:always_return_as_numpy: Argument `always_return_as_numpy` has been deprecated since version 1.5.0. It will be removed in version 1.7.0. The option is removed and the return type will always be equal to the input type.\n","  warn_deprecated(argname, msg, warning_category)\n","/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 2 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:332: UserWarning: the prediction of class 2 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 4 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","  2%|▏         | 1/52 [00:01<01:30,  1.77s/it]/tmp/ipython-input-4239864296.py:70: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  df = pd.read_csv(mask_path, header=None, delim_whitespace=True)\n","/usr/local/lib/python3.12/dist-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.metrics.utils get_mask_edges:always_return_as_numpy: Argument `always_return_as_numpy` has been deprecated since version 1.5.0. It will be removed in version 1.7.0. The option is removed and the return type will always be equal to the input type.\n","  warn_deprecated(argname, msg, warning_category)\n","/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 2 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:332: UserWarning: the prediction of class 2 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 6 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","  4%|▍         | 2/52 [00:01<00:39,  1.26it/s]/tmp/ipython-input-4239864296.py:70: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  df = pd.read_csv(mask_path, header=None, delim_whitespace=True)\n","/usr/local/lib/python3.12/dist-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.metrics.utils get_mask_edges:always_return_as_numpy: Argument `always_return_as_numpy` has been deprecated since version 1.5.0. It will be removed in version 1.7.0. The option is removed and the return type will always be equal to the input type.\n","  warn_deprecated(argname, msg, warning_category)\n","/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 2 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:332: UserWarning: the prediction of class 2 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","  6%|▌         | 3/52 [00:01<00:23,  2.08it/s]"]}]}]}