{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1Vga5GY4WwC37GQNLCpfbogiv-kiZvn-M","authorship_tag":"ABX9TyOs1pq111MqTpB/zAXqVjXA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5720c843b43a46b5b41f190eebce2a71":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fea9074e6a15488e92230f4b1e51e586","IPY_MODEL_beda645f78d84f5888511778470825f0","IPY_MODEL_02a5b9a3647346b4814d89853edc803d"],"layout":"IPY_MODEL_63df6b5ee76149af8c66a1ac96de070b"}},"fea9074e6a15488e92230f4b1e51e586":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_541574d8b1934802bcf3abb29c223606","placeholder":"​","style":"IPY_MODEL_80cb958c77a348f2b687e6c35c0e544e","value":"config.json: 100%"}},"beda645f78d84f5888511778470825f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f48f8777ef0247d28157af8360ec1af0","max":156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0301afaf661c4f5095d9f2570c352a1b","value":156}},"02a5b9a3647346b4814d89853edc803d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce22ed6e47544811bd8c4e9489b6ab36","placeholder":"​","style":"IPY_MODEL_239a46b7bfbd4470a7b64d80427e1d7c","value":" 156/156 [00:00&lt;00:00, 20.4kB/s]"}},"63df6b5ee76149af8c66a1ac96de070b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"541574d8b1934802bcf3abb29c223606":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80cb958c77a348f2b687e6c35c0e544e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f48f8777ef0247d28157af8360ec1af0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0301afaf661c4f5095d9f2570c352a1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce22ed6e47544811bd8c4e9489b6ab36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"239a46b7bfbd4470a7b64d80427e1d7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ac932d5071a4e72aef73030ff6043b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da157c7a08334143991f0cf1b02b436f","IPY_MODEL_4cf236593f374953a9119396300dfdf3","IPY_MODEL_4cf208bc37d84e339887acfa71ee090c"],"layout":"IPY_MODEL_ac7ab83917ac43c0b27c22ebafa678fc"}},"da157c7a08334143991f0cf1b02b436f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92b6ea7631ae4df98a13ef994ba859af","placeholder":"​","style":"IPY_MODEL_3d95b69c333f4f72b0402da781de06c7","value":"model.safetensors: 100%"}},"4cf236593f374953a9119396300dfdf3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be1dcb533fdc41aba94d750b3f942b5a","max":87275112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_732abf29e5dd49ac89038a163ad3c38b","value":87275112}},"4cf208bc37d84e339887acfa71ee090c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af7207613e4045c4b61b336bbb3e6eb6","placeholder":"​","style":"IPY_MODEL_34b5c3829c7e455c8b691f25069be70c","value":" 87.3M/87.3M [00:03&lt;00:00, 26.9MB/s]"}},"ac7ab83917ac43c0b27c22ebafa678fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92b6ea7631ae4df98a13ef994ba859af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d95b69c333f4f72b0402da781de06c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be1dcb533fdc41aba94d750b3f942b5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"732abf29e5dd49ac89038a163ad3c38b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af7207613e4045c4b61b336bbb3e6eb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34b5c3829c7e455c8b691f25069be70c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["Installs:"],"metadata":{"id":"lhdAoGpgmZNF"}},{"cell_type":"code","source":["!pip install -q segmentation-models-pytorch albumentations monai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y21_F-5Pk1Wg","executionInfo":{"status":"ok","timestamp":1764822300983,"user_tz":360,"elapsed":7362,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}},"outputId":"f194d722-e78c-4942-c936-7726a7ae2704"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["Imports:"],"metadata":{"id":"SiYhAOc1m1MQ"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from PIL import Image\n","from tqdm import tqdm\n","import random\n","\n","# Import advanced loss functions\n","from monai.losses import DiceLoss, HausdorffDTLoss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZwwT9uWLmxrC","executionInfo":{"status":"ok","timestamp":1764822363409,"user_tz":360,"elapsed":27699,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}},"outputId":"2430945f-b850-4219-ba6c-e8bbd94d19a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"]}]},{"cell_type":"code","source":["class Config:\n","    # -- Base Paths --\n","    # Points to the new COMBINED dataset (Original + Synthetic)\n","    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/\"\n","    DATA_PATH = os.path.join(BASE_PATH, \"combined_data\")\n","\n","    IMAGE_DIR = os.path.join(DATA_PATH, \"images\")\n","    MASK_DIR = os.path.join(DATA_PATH, \"masks\")\n","\n","    # Path to the Quality Control CSV\n","    QC_REPORT_CSV = os.path.join(BASE_PATH, \"missing_classes_from_mask.csv\")\n","\n","    # -- Output Paths --\n","    OUTPUT_DIR = os.path.join(BASE_PATH, \"outputs_v3\") # V3 Output\n","    OUTPUT_MASK_DIR = os.path.join(OUTPUT_DIR, \"pred_masks_unsqueezed\")\n","    COLOR_MASK_DIR = os.path.join(OUTPUT_DIR, \"color_masks_unsqueezed\")\n","\n","    # -- Data Split Sizes --\n","    # Since we are filtering data, the total available images might change (diff of 49 images).\n","    VAL_SIZE = 51\n","    TEST_SIZE = 52\n","    # Train size will be whatever is left\n","\n","    # -- Model Hyperparameters --\n","    ARCHITECTURE = 'unet'\n","    ENCODER = 'resnet34'\n","    ENCODER_WEIGHTS = 'imagenet'\n","    LEARNING_RATE = 1e-4\n","    OPTIMIZER = 'AdamW'\n","\n","    # -- Training Settings --\n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    BATCH_SIZE = 4\n","    NUM_EPOCHS = 30\n","    IMAGE_HEIGHT = 256\n","    IMAGE_WIDTH = 256\n","    NUM_CLASSES = 5\n","\n","    # -- LOSS CONFIGURATION (Successful V2 Settings) --\n","    # Weights: [Background, Stem, Leaf, Root, Seed]\n","    # We use the moderate weights that worked well\n","    CLASS_WEIGHTS = torch.tensor([\n","        1.0,  # Background\n","        7.0,  # Stem\n","        5.0,  # Leaf\n","        5.0,  # Root\n","        7.0   # Seed\n","    ], device=DEVICE)\n","\n","    # Weights for the hybrid loss (Pure Weighted Dice)\n","    HYBRID_WEIGHT_DICE = 1.0\n","    HYBRID_WEIGHT_HD = 0.0\n","\n","    # -- Visualization --\n","    COLOR_MAP = {\n","        0: (0, 0, 0),         # background - black\n","        1: (139, 69, 19),     # stem - brown\n","        2: (0, 255, 0),       # leaf - green\n","        3: (255, 255, 0),     # root - yellow\n","        4: (255, 0, 0),       # seed - red\n","    }\n","\n","os.makedirs(Config.OUTPUT_MASK_DIR, exist_ok=True)\n","os.makedirs(Config.COLOR_MASK_DIR, exist_ok=True)"],"metadata":{"id":"laF04Z4Rm7-F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loads all images, excludes those marked FALSE in the CSV, and splits the rest."],"metadata":{"id":"rPwdFsHUpTI2"}},{"cell_type":"code","source":["def get_filtered_data_splits(image_dir, qc_csv_path, val_size, test_size):\n","    # 1. Get all available images in the combined folder\n","    all_images = sorted([f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))])\n","    print(f\"Total images found in folder: {len(all_images)}\")\n","\n","    # 2. Load the unsafe image list\n","    excluded_filenames = set()\n","    if os.path.exists(qc_csv_path):\n","        df = pd.read_csv(qc_csv_path)\n","        # We look for rows where Mask_correct is FALSE (boolean or string)\n","        bad_rows = df[df['Mask_correct'].astype(str).str.upper() == 'FALSE']\n","        excluded_filenames = set(bad_rows['filename'].tolist())\n","        print(f\"Found {len(excluded_filenames)} images marked as FALSE in CSV to exclude.\")\n","    else:\n","        print(\"Warning:CSV not found. Proceeding without filtering.\")\n","\n","    # 3. Filter the list\n","    valid_images = []\n","    for img_name in all_images:\n","        # We need to check the MASK name against the blacklist\n","        # Assuming standard naming: image.jpg -> image_mask.png\n","        mask_name = os.path.splitext(img_name)[0] + \"_mask.png\"\n","\n","        if mask_name in excluded_filenames:\n","            continue # Skip this image\n","\n","        valid_images.append(img_name)\n","\n","    print(f\"Total Valid Images for Training: {len(valid_images)}\")\n","\n","    # 4. Shuffle and Split\n","    random.seed(42)\n","    random.shuffle(valid_images)\n","\n","    # Prioritize Test and Val sets, give rest to Train\n","    if len(valid_images) < (val_size + test_size):\n","        raise ValueError(\"Not enough valid images to create Validation and Test sets!\")\n","\n","    test_files = valid_images[:test_size]\n","    val_files = valid_images[test_size : test_size + val_size]\n","    train_files = valid_images[test_size + val_size :]\n","\n","    print(f\"Training set size: {len(train_files)}\")\n","    print(f\"Validation set size: {len(val_files)}\")\n","    print(f\"Test set size: {len(test_files)}\")\n","\n","    return train_files, val_files, test_files"],"metadata":{"id":"Z43t0tCPpMbY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PlantDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, image_filenames, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.images = image_filenames\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        img_name = self.images[index]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        mask_name = os.path.splitext(img_name)[0] + \"_mask.png\"\n","        mask_path = os.path.join(self.mask_dir, mask_name)\n","\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n","\n","        # Get original dimensions for unsqueezing\n","        original_height, original_width = image.shape[:2]\n","\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        # Clip mask to be safe (0 to 4)\n","        mask[mask >= Config.NUM_CLASSES] = 0\n","\n","        return image, mask.long(), (original_height, original_width)\n","\n","# Augmentations\n","train_transform = A.Compose([\n","    A.Resize(height=Config.IMAGE_HEIGHT, width=Config.IMAGE_WIDTH),\n","    A.Rotate(limit=35, p=0.5),\n","    A.HorizontalFlip(p=0.5),\n","    A.VerticalFlip(p=0.5),\n","    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n","    ToTensorV2(),\n","])\n","\n","val_transform = A.Compose([\n","    A.Resize(height=Config.IMAGE_HEIGHT, width=Config.IMAGE_WIDTH),\n","    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n","    ToTensorV2(),\n","])"],"metadata":{"id":"JUPtqRZ0qHbY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class WeightedHybridLoss(nn.Module):\n","    def __init__(self, class_weights, weight_dice=1.0, weight_hd=0.0):\n","        super(WeightedHybridLoss, self).__init__()\n","        self.w_dice = weight_dice\n","        self.w_hd = weight_hd\n","        self.class_weights = class_weights\n","\n","        self.dice_loss = DiceLoss(\n","            softmax=True, to_onehot_y=True, include_background=True, reduction='none'\n","        )\n","        self.hd_loss = HausdorffDTLoss(\n","            softmax=True, to_onehot_y=True, include_background=True, reduction='none'\n","        )\n","\n","    def forward(self, preds_logits, targets_idx):\n","        targets_idx = targets_idx.unsqueeze(1)\n","\n","        loss_dice_per_class = self.dice_loss(preds_logits, targets_idx)\n","\n","        # Apply weights to Dice\n","        weighted_loss_dice_all = loss_dice_per_class * self.class_weights\n","        weighted_loss_dice = weighted_loss_dice_all.mean()\n","\n","        total_loss = self.w_dice * weighted_loss_dice\n","\n","        # Add HD loss if weight > 0 (Currently 0 in Config)\n","        if self.w_hd > 0:\n","            loss_hd_per_class = self.hd_loss(preds_logits, targets_idx)\n","            weighted_loss_hd_all = loss_hd_per_class * self.class_weights\n","            weighted_loss_hd = weighted_loss_hd_all.mean()\n","            total_loss += (self.w_hd * weighted_loss_hd)\n","\n","        return total_loss"],"metadata":{"id":"A5Z6FwZmq7-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_fn(loader, model, optimizer, loss_fn, scaler):\n","    loop = tqdm(loader, desc=\"Training\")\n","    total_loss = 0\n","\n","    for batch_idx, (data, targets, _) in enumerate(loop):\n","        data = data.to(device=Config.DEVICE)\n","        targets = targets.to(device=Config.DEVICE)\n","\n","        with torch.amp.autocast('cuda'):\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","        loop.set_postfix(loss=loss.item())\n","\n","    return total_loss / len(loader)"],"metadata":{"id":"XNU4PXK4rDZ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval_fn(loader, model, loss_fn):\n","    model.eval()\n","    total_loss = 0\n","    loop = tqdm(loader, desc=\"Validation\")\n","\n","    with torch.no_grad():\n","        for data, targets, _ in loop:\n","            data = data.to(device=Config.DEVICE)\n","            targets = targets.to(device=Config.DEVICE)\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","            total_loss += loss.item()\n","            loop.set_postfix(val_loss=loss.item())\n","\n","    model.train()\n","    return total_loss / len(loader)"],"metadata":{"id":"gi0AGcaorOhV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mask_to_rgb(mask_tensor, color_map):\n","    mask = mask_tensor.cpu().numpy().squeeze()\n","    rgb_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n","    for class_idx, color in color_map.items():\n","        rgb_mask[mask == class_idx] = color\n","    return Image.fromarray(rgb_mask)"],"metadata":{"id":"sQi0zxrqrXg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_predictions_fn(loader, model, folder_basename=\"\"):\n","    print(f\"\\n--- Saving predictions for {folder_basename} set ---\")\n","    model.eval()\n","\n","    # Define sub-folders for output\n","    output_mask_dir = os.path.join(Config.OUTPUT_MASK_DIR, folder_basename)\n","    color_mask_dir = os.path.join(Config.COLOR_MASK_DIR, folder_basename)\n","    os.makedirs(output_mask_dir, exist_ok=True)\n","    os.makedirs(color_mask_dir, exist_ok=True)\n","\n","    for idx in tqdm(range(len(loader.dataset)), desc=f\"Saving {folder_basename} Predictions\"):\n","        img_tensor, _, (original_h, original_w) = loader.dataset[idx]\n","\n","        with torch.no_grad():\n","            img_tensor = img_tensor.to(Config.DEVICE).unsqueeze(0)\n","            preds = model(img_tensor)\n","            final_mask_tensor = torch.argmax(preds, dim=1).squeeze(0)\n","\n","        pred_mask_np = final_mask_tensor.cpu().numpy().astype(np.uint8)\n","\n","        # Unsqueeze\n","        resized_mask = cv2.resize(\n","            pred_mask_np,\n","            (original_w, original_h),\n","            interpolation=cv2.INTER_NEAREST\n","        )\n","\n","        # Save Raw Mask\n","        pred_mask_img = Image.fromarray(resized_mask)\n","        original_filename = loader.dataset.images[idx]\n","        mask_filename = os.path.splitext(original_filename)[0] + \"_mask.png\"\n","        pred_mask_img.save(os.path.join(output_mask_dir, mask_filename))\n","\n","        # Save Color Mask\n","        color_mask_img = mask_to_rgb(torch.from_numpy(resized_mask), Config.COLOR_MAP)\n","        color_mask_img.save(os.path.join(color_mask_dir, mask_filename))\n","\n","    model.train()"],"metadata":{"id":"vHRYvjC2ra_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    print(f\"Using device: {Config.DEVICE}\")\n","    print(f\"Dataset Path: {Config.DATA_PATH}\")\n","\n","    # --- 1. Get Filtered Splits ---\n","    train_files, val_files, test_files = get_filtered_data_splits(\n","        Config.IMAGE_DIR, Config.QC_REPORT_CSV, Config.VAL_SIZE, Config.TEST_SIZE\n","    )\n","\n","    # --- 2. Create Loaders ---\n","    train_dataset = PlantDataset(Config.IMAGE_DIR, Config.MASK_DIR, train_files, train_transform)\n","    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n","\n","    val_dataset = PlantDataset(Config.IMAGE_DIR, Config.MASK_DIR, val_files, val_transform)\n","    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n","\n","    test_dataset = PlantDataset(Config.IMAGE_DIR, Config.MASK_DIR, test_files, val_transform)\n","    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","    # --- 3. Model & Loss ---\n","    model = smp.create_model(\n","        arch=Config.ARCHITECTURE,\n","        encoder_name=Config.ENCODER,\n","        encoder_weights=Config.ENCODER_WEIGHTS,\n","        in_channels=3,\n","        classes=Config.NUM_CLASSES,\n","    ).to(Config.DEVICE)\n","\n","    print(\"\\n--- Initializing V3 Loss Function ---\")\n","    loss_fn = WeightedHybridLoss(\n","        class_weights=Config.CLASS_WEIGHTS,\n","        weight_dice=Config.HYBRID_WEIGHT_DICE,\n","        weight_hd=Config.HYBRID_WEIGHT_HD\n","    )\n","    print(f\"Class Weights: {Config.CLASS_WEIGHTS.cpu().numpy()}\")\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE)\n","    scaler = torch.amp.GradScaler('cuda')\n","    best_val_loss = float('inf')\n","\n","    # --- 4. Training Loop ---\n","    for epoch in range(Config.NUM_EPOCHS):\n","        print(f\"\\n--- Epoch {epoch+1}/{Config.NUM_EPOCHS} ---\")\n","        train_loss = train_fn(train_loader, model, optimizer, loss_fn, scaler)\n","        val_loss = eval_fn(val_loader, model, loss_fn)\n","\n","        print(f\"Average Train Loss: {train_loss:.4f}\")\n","        print(f\"Average Val Loss: {val_loss:.4f}\")\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), os.path.join(Config.BASE_PATH, \"best_model_v3.pth\"))\n","            print(\"=> Saved new best model\")\n","\n","    # --- 5. Save Test Predictions ---\n","    print(\"\\n--- Loading best model for final testing ---\")\n","    model.load_state_dict(torch.load(os.path.join(Config.BASE_PATH, \"best_model_v3.pth\")))\n","    save_predictions_fn(test_loader, model, folder_basename=\"test_set\")\n","    print(\"--- V3 Training Complete ---\")"],"metadata":{"id":"M-55AQlOrtj1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5720c843b43a46b5b41f190eebce2a71","fea9074e6a15488e92230f4b1e51e586","beda645f78d84f5888511778470825f0","02a5b9a3647346b4814d89853edc803d","63df6b5ee76149af8c66a1ac96de070b","541574d8b1934802bcf3abb29c223606","80cb958c77a348f2b687e6c35c0e544e","f48f8777ef0247d28157af8360ec1af0","0301afaf661c4f5095d9f2570c352a1b","ce22ed6e47544811bd8c4e9489b6ab36","239a46b7bfbd4470a7b64d80427e1d7c","4ac932d5071a4e72aef73030ff6043b3","da157c7a08334143991f0cf1b02b436f","4cf236593f374953a9119396300dfdf3","4cf208bc37d84e339887acfa71ee090c","ac7ab83917ac43c0b27c22ebafa678fc","92b6ea7631ae4df98a13ef994ba859af","3d95b69c333f4f72b0402da781de06c7","be1dcb533fdc41aba94d750b3f942b5a","732abf29e5dd49ac89038a163ad3c38b","af7207613e4045c4b61b336bbb3e6eb6","34b5c3829c7e455c8b691f25069be70c"]},"id":"1K1snmpBr5Ue","executionInfo":{"status":"ok","timestamp":1764824435064,"user_tz":360,"elapsed":763297,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}},"outputId":"7fc1a930-d130-40ac-840c-ed194a81ecb5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Dataset Path: /content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/combined_data\n","Total images found in folder: 595\n","Found 32 images marked as FALSE in CSV to exclude.\n","Total Valid Images for Training: 563\n","Training set size: 460\n","Validation set size: 51\n","Test set size: 52\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5720c843b43a46b5b41f190eebce2a71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ac932d5071a4e72aef73030ff6043b3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- Initializing V3 Loss Function ---\n","Class Weights: [1. 7. 5. 5. 7.]\n","\n","--- Epoch 1/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [06:21<00:00,  3.32s/it, loss=3.7]\n","Validation: 100%|██████████| 13/13 [00:39<00:00,  3.05s/it, val_loss=4.04]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 4.3135\n","Average Val Loss: 3.7911\n","=> Saved new best model\n","\n","--- Epoch 2/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 14.33it/s, loss=3.41]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 24.52it/s, val_loss=3.17]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 3.5842\n","Average Val Loss: 3.0804\n","=> Saved new best model\n","\n","--- Epoch 3/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 13.85it/s, loss=3.1]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.31it/s, val_loss=2.85]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 3.0366\n","Average Val Loss: 2.8079\n","=> Saved new best model\n","\n","--- Epoch 4/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 13.76it/s, loss=2.64]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.56it/s, val_loss=2.64]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 2.8407\n","Average Val Loss: 2.7043\n","=> Saved new best model\n","\n","--- Epoch 5/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.25it/s, loss=2.71]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.26it/s, val_loss=2.63]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 2.7526\n","Average Val Loss: 2.6528\n","=> Saved new best model\n","\n","--- Epoch 6/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 14.09it/s, loss=2.67]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 24.31it/s, val_loss=2.47]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 2.6647\n","Average Val Loss: 2.5042\n","=> Saved new best model\n","\n","--- Epoch 7/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.40it/s, loss=2.29]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.95it/s, val_loss=1.96]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 2.3966\n","Average Val Loss: 2.2028\n","=> Saved new best model\n","\n","--- Epoch 8/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 14.25it/s, loss=2.11]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.79it/s, val_loss=1.86]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 2.1797\n","Average Val Loss: 2.1293\n","=> Saved new best model\n","\n","--- Epoch 9/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.10it/s, loss=1.92]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.88it/s, val_loss=1.8]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 2.0932\n","Average Val Loss: 2.0099\n","=> Saved new best model\n","\n","--- Epoch 10/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 14.26it/s, loss=2.25]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.27it/s, val_loss=1.73]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 2.0366\n","Average Val Loss: 2.0159\n","\n","--- Epoch 11/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.42it/s, loss=1.91]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 23.96it/s, val_loss=1.74]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.9944\n","Average Val Loss: 1.9860\n","=> Saved new best model\n","\n","--- Epoch 12/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 13.95it/s, loss=1.99]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.48it/s, val_loss=1.75]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.9748\n","Average Val Loss: 1.9690\n","=> Saved new best model\n","\n","--- Epoch 13/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.06it/s, loss=2.12]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 24.38it/s, val_loss=1.55]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.9137\n","Average Val Loss: 1.7627\n","=> Saved new best model\n","\n","--- Epoch 14/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 14.17it/s, loss=1.87]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.97it/s, val_loss=1.55]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.7910\n","Average Val Loss: 1.5326\n","=> Saved new best model\n","\n","--- Epoch 15/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.36it/s, loss=1.81]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.68it/s, val_loss=1.14]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.6321\n","Average Val Loss: 1.4085\n","=> Saved new best model\n","\n","--- Epoch 16/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 14.36it/s, loss=1.51]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 24.98it/s, val_loss=1.11]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.5207\n","Average Val Loss: 1.4077\n","=> Saved new best model\n","\n","--- Epoch 17/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.23it/s, loss=1.38]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.96it/s, val_loss=1.44]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.3084\n","Average Val Loss: 1.3009\n","=> Saved new best model\n","\n","--- Epoch 18/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 14.31it/s, loss=0.993]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.31it/s, val_loss=0.925]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.2645\n","Average Val Loss: 1.2515\n","=> Saved new best model\n","\n","--- Epoch 19/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.15it/s, loss=1.17]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.86it/s, val_loss=1.04]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.2489\n","Average Val Loss: 1.2832\n","\n","--- Epoch 20/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 12.79it/s, loss=1.27]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.32it/s, val_loss=0.853]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.2013\n","Average Val Loss: 1.1155\n","=> Saved new best model\n","\n","--- Epoch 21/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 13.47it/s, loss=1.39]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 23.80it/s, val_loss=0.869]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.1883\n","Average Val Loss: 1.1284\n","\n","--- Epoch 22/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.26it/s, loss=1.1]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.14it/s, val_loss=0.852]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.1773\n","Average Val Loss: 1.1518\n","\n","--- Epoch 23/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 13.88it/s, loss=1.64]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 26.29it/s, val_loss=0.904]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.1559\n","Average Val Loss: 1.1125\n","=> Saved new best model\n","\n","--- Epoch 24/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 13.57it/s, loss=1.53]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 24.04it/s, val_loss=0.852]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.1436\n","Average Val Loss: 1.1276\n","\n","--- Epoch 25/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.16it/s, loss=0.84]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.37it/s, val_loss=0.748]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.1343\n","Average Val Loss: 1.1131\n","\n","--- Epoch 26/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 14.05it/s, loss=1.37]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.75it/s, val_loss=0.728]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.1207\n","Average Val Loss: 1.1060\n","=> Saved new best model\n","\n","--- Epoch 27/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 13.54it/s, loss=1]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 24.99it/s, val_loss=0.804]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.1175\n","Average Val Loss: 1.0860\n","=> Saved new best model\n","\n","--- Epoch 28/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:10<00:00, 11.20it/s, loss=1.36]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.58it/s, val_loss=0.874]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.1062\n","Average Val Loss: 1.1363\n","\n","--- Epoch 29/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 13.93it/s, loss=1]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 27.18it/s, val_loss=1.04]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.1011\n","Average Val Loss: 1.0977\n","\n","--- Epoch 30/30 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 115/115 [00:08<00:00, 14.29it/s, loss=1.22]\n","Validation: 100%|██████████| 13/13 [00:00<00:00, 25.32it/s, val_loss=0.872]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 1.0881\n","Average Val Loss: 1.0662\n","=> Saved new best model\n","\n","--- Loading best model for final testing ---\n","\n","--- Saving predictions for test_set set ---\n"]},{"output_type":"stream","name":"stderr","text":["Saving test_set Predictions: 100%|██████████| 52/52 [00:50<00:00,  1.02it/s]"]},{"output_type":"stream","name":"stdout","text":["--- V3 Training Complete ---\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# =================================================================================\n","# 0. SETUP AND IMPORTS\n","# =================================================================================\n","!pip install -q monai pandas\n","\n","import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","from tqdm import tqdm\n","from monai.metrics import (\n","    compute_dice,\n","    compute_iou,\n","    compute_hausdorff_distance\n",")\n","\n","# =================================================================================\n","# 1. CONFIGURATION\n","# =================================================================================\n","# --- PATHS FOR V3 ---\n","# Ground Truth Masks (The COMBINED dataset masks)\n","GT_MASK_DIR = \"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/combined_data/masks/\"\n","\n","# Predicted Masks (From V3 Output)\n","PRED_MASK_DIR = \"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/outputs_v3/pred_masks_unsqueezed/test_set/\"\n","\n","# --- CORRECTED CLASS MAP ---\n","NUM_CLASSES = 5\n","CLASS_MAP = {\n","    0: \"Background\",\n","    1: \"Stem\",\n","    2: \"Leaf\",\n","    3: \"Root\",\n","    4: \"Seed\",\n","}\n","\n","# =================================================================================\n","# 2. HELPER FUNCTIONS\n","# =================================================================================\n","def to_one_hot(mask, num_classes):\n","    \"\"\"Converts a (H, W) mask to (1, C, H, W) one-hot tensor.\"\"\"\n","    # Clip values just in case\n","    mask[mask >= num_classes] = 0\n","\n","    one_hot = np.eye(num_classes)[mask] # (H, W, C)\n","    one_hot = np.transpose(one_hot, (2, 0, 1)) # (C, H, W)\n","    return torch.from_numpy(one_hot).unsqueeze(0) # (1, C, H, W)\n","\n","# =================================================================================\n","# 3. MAIN ANALYSIS LOOP\n","# =================================================================================\n","def run_analysis():\n","    print(\"Starting V3 Analysis...\")\n","    print(f\"GT Directory: {GT_MASK_DIR}\")\n","    print(f\"Pred Directory: {PRED_MASK_DIR}\")\n","\n","    results_list = []\n","    pred_files = [f for f in os.listdir(PRED_MASK_DIR) if f.endswith('.png')]\n","\n","    if len(pred_files) == 0:\n","        print(\"Error: No prediction files found! Check your PRED_MASK_DIR path.\")\n","        return\n","\n","    for filename in tqdm(pred_files):\n","        pred_path = os.path.join(PRED_MASK_DIR, filename)\n","\n","        # Logic to find GT file (Assumes names match exactly or close to it)\n","        # V3 script saves predictions as \"original_name_mask.png\"\n","        gt_path = os.path.join(GT_MASK_DIR, filename)\n","\n","        if not os.path.exists(gt_path):\n","            print(f\"Skipping {filename}: GT mask not found.\")\n","            continue\n","\n","        # Load Masks\n","        gt_mask = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n","        pred_mask = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if gt_mask is None or pred_mask is None:\n","            continue\n","\n","        # Resize GT if dimensions don't match (Safety check for unsqueezing issues)\n","        if gt_mask.shape != pred_mask.shape:\n","            # We assume prediction is correct size (unsqueezed), so we resize GT to match\n","            # This handles cases where GT might be slightly off due to pre-processing\n","            gt_mask = cv2.resize(gt_mask, (pred_mask.shape[1], pred_mask.shape[0]), interpolation=cv2.INTER_NEAREST)\n","\n","        # Convert to One-Hot\n","        gt_onehot = to_one_hot(gt_mask, NUM_CLASSES)\n","        pred_onehot = to_one_hot(pred_mask, NUM_CLASSES)\n","\n","        # --- Calculate Metrics ---\n","        dice = compute_dice(pred_onehot, gt_onehot, include_background=True)\n","        iou = compute_iou(pred_onehot, gt_onehot, include_background=True)\n","        hd95 = compute_hausdorff_distance(pred_onehot, gt_onehot, include_background=True, percentile=95)\n","\n","        # Store results\n","        file_metrics = {'filename': filename}\n","        for i in range(NUM_CLASSES):\n","            c_name = CLASS_MAP[i]\n","            file_metrics[f\"{c_name}_Dice\"] = dice[0, i].item()\n","            file_metrics[f\"{c_name}_IOU\"] = iou[0, i].item()\n","            file_metrics[f\"{c_name}_HD95\"] = hd95[0, i].item()\n","\n","        results_list.append(file_metrics)\n","\n","    # =================================================================================\n","    # 4. REPORTING\n","    # =================================================================================\n","    if not results_list:\n","        print(\"No results generated.\")\n","        return\n","\n","    df = pd.DataFrame(results_list)\n","\n","    # Calculate Averages\n","    overall_stats = df.mean(numeric_only=True)\n","\n","    print(\"\\n\\n--- V3 Overall Average Statistics (Test Set) ---\")\n","    summary_data = []\n","    for i in range(NUM_CLASSES):\n","        c_name = CLASS_MAP[i]\n","        summary_data.append({\n","            \"Class\": c_name,\n","            \"Dice (↑)\": overall_stats.get(f\"{c_name}_Dice\"),\n","            \"IOU (↑)\": overall_stats.get(f\"{c_name}_IOU\"),\n","            \"HD95 (↓)\": overall_stats.get(f\"{c_name}_HD95\"),\n","        })\n","\n","    summary_df = pd.DataFrame(summary_data)\n","    print(summary_df.to_markdown(index=False, floatfmt=\".4f\"))\n","\n","    # Optional: Save to CSV for comparison later\n","    summary_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/v3_results_summary.csv\", index=False)\n","    print(\"\\nSummary saved to v3_results_summary.csv\")\n","\n","if __name__ == \"__main__\":\n","    run_analysis()"],"metadata":{"id":"8ILf-v5Dvrt2","executionInfo":{"status":"ok","timestamp":1764824652094,"user_tz":360,"elapsed":8376,"user":{"displayName":"Advait Tilak","userId":"04081556924397245104"}},"outputId":"cb9fc015-ae61-4dcc-b062-45fadcd6bda7","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting V3 Analysis...\n","GT Directory: /content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/combined_data/masks/\n","Pred Directory: /content/drive/MyDrive/Colab Notebooks/phenocyte_seg/phenocyte_seg/outputs_v3/pred_masks_unsqueezed/test_set/\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/52 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.metrics.utils get_mask_edges:always_return_as_numpy: Argument `always_return_as_numpy` has been deprecated since version 1.5.0. It will be removed in version 1.7.0. The option is removed and the return type will always be equal to the input type.\n","  warn_deprecated(argname, msg, warning_category)\n","  2%|▏         | 1/52 [00:00<00:06,  7.72it/s]/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 3 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n"," 25%|██▌       | 13/52 [00:01<00:03, 11.58it/s]/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 4 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n"," 29%|██▉       | 15/52 [00:01<00:03, 11.96it/s]/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:332: UserWarning: the prediction of class 4 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 1 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n"," 48%|████▊     | 25/52 [00:02<00:02, 12.36it/s]/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:327: UserWarning: the ground truth of class 2 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/monai/metrics/utils.py:332: UserWarning: the prediction of class 2 is all 0, this may result in nan/inf distance.\n","  warnings.warn(\n","100%|██████████| 52/52 [00:04<00:00, 12.43it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","\n","--- V3 Overall Average Statistics (Test Set) ---\n","| Class      |   Dice (↑) |   IOU (↑) |   HD95 (↓) |\n","|:-----------|-----------:|----------:|-----------:|\n","| Background |     0.9942 |    0.9885 |     7.1458 |\n","| Stem       |     0.7149 |    0.5774 |     7.1714 |\n","| Leaf       |     0.8360 |    0.7231 |     7.7421 |\n","| Root       |     0.8103 |    0.6908 |    10.4944 |\n","| Seed       |     0.6350 |    0.5143 |    20.8207 |\n","\n","Summary saved to v3_results_summary.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}